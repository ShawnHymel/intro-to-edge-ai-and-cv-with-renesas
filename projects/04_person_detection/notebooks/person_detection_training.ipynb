{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PBtFsbroE-Q"
      },
      "source": [
        "# Person Detection Training\n",
        "\n",
        "In this notebook, we are going to use transfer learning to update a pre-trained [FastestDet](https://github.com/dog-qiuqiu/FastestDet) model to identify people in an image. We will retrain the model using a preexisting [dataset from Kaggle](https://www.kaggle.com/datasets/adilshamim8/people-detection/).\n",
        "\n",
        "In Google Colab, select **File > Open notebook** then select the **Upload** tab. Select this file to open it in Colab.\n",
        "\n",
        "Press **shift + enter** to execute each cell in order. Make sure you stop and read each text section, as there are some manual steps you will need to perform (e.g. upload dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2yAdGHQmlH-"
      },
      "outputs": [],
      "source": [
        "# Install specific versions of the packages\n",
        "!python3 -m pip install \\\n",
        "    opencv-python=='4.13.0.90' \\\n",
        "    matplotlib=='3.10.0' \\\n",
        "    numpy=='2.0.2' \\\n",
        "    onnxscript=='0.5.7' \\\n",
        "    pandas=='2.2.2' \\\n",
        "    Pillow=='11.3.0' \\\n",
        "    torch=='2.9.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2rpPSBwoqfo"
      },
      "outputs": [],
      "source": [
        "# Import standard libraries\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "import types\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# Import third-party libraries\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N88kZ_nao0lu"
      },
      "outputs": [],
      "source": [
        "# Print out the versions of the libraries\n",
        "print(f\"OpenCV version: {cv2.__version__}\")\n",
        "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Pillow version: {Image.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dltaJwHBo195"
      },
      "outputs": [],
      "source": [
        "# General settings\n",
        "SEED = 42\n",
        "DATASET_URL = \"https://www.kaggle.com/api/v1/datasets/download/adilshamim8/people-detection\"\n",
        "DATASET_ZIP_PATH = Path(\"/content/dataset.zip\")\n",
        "DATASET_PATH = Path(\"/content/dataset\")\n",
        "CONFIG_PATH = Path(\"/content/yolo_config\")\n",
        "\n",
        "# Pre-trained model settings\n",
        "REPO_URL = \"https://github.com/dog-qiuqiu/Yolo-Fastest/archive/refs/tags/v.1.1.0.zip\"\n",
        "REPO_ZIP_PATH = Path(\"/content/Yolo-Fastest.zip\")\n",
        "REPO_PATH = Path(\"/content/Yolo-Fastest\")\n",
        "WEIGHTS_PATH = REPO_PATH\n",
        "MODEL_CFG_PATH = REPO_PATH / \"ModelZoo/yolo-fastest-1.1_coco/yolo-fastest-1.1.cfg\"\n",
        "MODEL_WEIGHTS_PATH = REPO_PATH / \"ModelZoo/yolo-fastest-1.1_coco/yolo-fastest-1.1.weights\"\n",
        "MODIFIED_CFG_PATH = Path(\"/content/yolo-fastest-1.1_192_no_dropout.cfg\")\n",
        "\n",
        "# Darknet to PyTorch converter script\n",
        "CONVERTER_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/intro-to-edge-ai-and-cv-with-renesas/main/scripts/yolo_fastest_to_pytorch.py\"\n",
        "CONVERTER_SCRIPT_PATH = Path(\"/content/yolo_fastest_to_pytorch.py\")\n",
        "\n",
        "# Utilities script\n",
        "UTILS_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/intro-to-edge-ai-and-cv-with-renesas/main/scripts/yolo_fastest_utils.py\"\n",
        "UTILS_SCRIPT_PATH = Path(\"/content/yolo_fastest_utils.py\")\n",
        "\n",
        "# Loss calculation script\n",
        "LOSS_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/intro-to-edge-ai-and-cv-with-renesas/main/scripts/yolo_fastest_loss.py\"\n",
        "LOSS_SCRIPT_PATH = Path(\"/content/yolo_fastest_loss.py\")\n",
        "\n",
        "# Image preprocessing settings\n",
        "IMG_WIDTH = 192\n",
        "IMG_HEIGHT = 192\n",
        "\n",
        "# Model settings\n",
        "NUM_ANCHORS = 3\n",
        "NUM_CLASSES = 1\n",
        "NEW_FILTERS = NUM_ANCHORS * (5 + NUM_CLASSES)  # 3 * 6 = 18\n",
        "\n",
        "# Model update settings\n",
        "NUM_CLASSES = 1\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2 # Number of parallel processes for loading data\n",
        "\n",
        "# Freeze settings (set to True to freeze)\n",
        "FREEZE_BACKBONE = False\n",
        "FREEZE_NECK = False\n",
        "\n",
        "# Training settings\n",
        "NUM_EPOCHS = 20\n",
        "LEARNING_RATE = 0.0001\n",
        "BEST_MODEL_PATH = Path(\"/content/best_model.pth\")\n",
        "\n",
        "# ONNX export settings\n",
        "ONNX_OPSET_VERSION = 18\n",
        "ONNX_PATH = Path(\"/content/model.onnx\")\n",
        "\n",
        "# Calibration data settings\n",
        "NUM_CALIB_SAMPLES = 20\n",
        "CALIB_NPZ_PATH = Path(\"/content/calibration_data.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsM8qVjgZ5xR"
      },
      "outputs": [],
      "source": [
        " # Set random seeds for reproducibility\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiJIRY9UZ6iW"
      },
      "outputs": [],
      "source": [
        "# Define the target compute device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxfxp3h_pnT0"
      },
      "source": [
        "## Prepare Dataset\n",
        "\n",
        "Rather than create our own (which is very time consuming), we will use an existing dataset to retrain the model.\n",
        "\n",
        "We first download and unzip the dataset. After that, we covert the annotations to YOLO-style (normalized bounding box information in a .txt file for each image). Then, we create a few text files in *yolo_config/* that lists the classes and gives the location of each image file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGgjIUBAppNp"
      },
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "!curl -L -o {DATASET_ZIP_PATH} {DATASET_URL}\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(DATASET_ZIP_PATH, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATASET_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUdT2_5VuDLV"
      },
      "outputs": [],
      "source": [
        "def convert_csv_to_yolo(csv_path, class_mapping={'person': 0}):\n",
        "    \"\"\"\n",
        "    Convert CSV annotations to YOLO format.\n",
        "    Writes .txt files alongside original images (in the same folder).\n",
        "    Creates empty .txt files for images not in the CSV (negative samples).\n",
        "\n",
        "    CSV format: filename, width, height, class, xmin, ymin, xmax, ymax\n",
        "    YOLO format: class_id cx cy w h (normalized 0-1)\n",
        "\n",
        "    Returns list of image paths for train.txt/val.txt generation.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    source_dir = csv_path.parent\n",
        "\n",
        "    image_paths = []\n",
        "    annotated_stems = set()  # Track which images we've processed\n",
        "\n",
        "    # Group by filename since one image can have multiple annotations\n",
        "    grouped = df.groupby('filename')\n",
        "\n",
        "    for filename, group in grouped:\n",
        "        # Get image dimensions (same for all rows of this image)\n",
        "        img_width = group.iloc[0]['width']\n",
        "        img_height = group.iloc[0]['height']\n",
        "\n",
        "        # Convert each bounding box to YOLO format\n",
        "        yolo_annotations = []\n",
        "        for _, row in group.iterrows():\n",
        "            class_id = class_mapping[row['class']]\n",
        "\n",
        "            # Convert xmin,ymin,xmax,ymax to cx,cy,w,h (normalized)\n",
        "            xmin, ymin, xmax, ymax = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
        "\n",
        "            cx = ((xmin + xmax) / 2) / img_width\n",
        "            cy = ((ymin + ymax) / 2) / img_height\n",
        "            w = (xmax - xmin) / img_width\n",
        "            h = (ymax - ymin) / img_height\n",
        "\n",
        "            # Clamp values to [0, 1] just in case\n",
        "            cx = max(0, min(1, cx))\n",
        "            cy = max(0, min(1, cy))\n",
        "            w = max(0, min(1, w))\n",
        "            h = max(0, min(1, h))\n",
        "\n",
        "            yolo_annotations.append(f\"{class_id} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\")\n",
        "\n",
        "        # Write YOLO annotation file alongside the image\n",
        "        txt_filename = Path(filename).stem + '.txt'\n",
        "        txt_path = source_dir / txt_filename\n",
        "        with open(txt_path, 'w') as f:\n",
        "            f.write('\\n'.join(yolo_annotations))\n",
        "\n",
        "        # Store full image path and track stem\n",
        "        image_paths.append(str(source_dir / filename))\n",
        "        annotated_stems.add(Path(filename).stem)\n",
        "\n",
        "    # Create empty .txt files for images not in CSV (negative samples)\n",
        "    negative_count = 0\n",
        "    for img_file in source_dir.iterdir():\n",
        "        if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "            if img_file.stem not in annotated_stems:\n",
        "                txt_path = img_file.with_suffix('.txt')\n",
        "                txt_path.touch()  # Create empty file\n",
        "                image_paths.append(str(img_file))\n",
        "                negative_count += 1\n",
        "\n",
        "    print(f\"Positive samples: {len(annotated_stems)}, Negative samples: {negative_count}\")\n",
        "\n",
        "    return image_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0lSXTHg0mAE"
      },
      "outputs": [],
      "source": [
        "# Define the dataset splits\n",
        "splits = {\n",
        "    'train': DATASET_PATH / 'train' / 'train' / '_annotations.csv',\n",
        "    'val': DATASET_PATH / 'valid' / 'valid' / '_annotations.csv',\n",
        "    'test': DATASET_PATH / 'test' / 'test' / '_annotations.csv',\n",
        "}\n",
        "\n",
        "# Read the CSV file for each split and add YOLO-style .txt files for each image\n",
        "all_paths = {}\n",
        "for split_name, csv_path in splits.items():\n",
        "    print(f\"Converting {split_name}...\")\n",
        "    paths = convert_csv_to_yolo(csv_path)\n",
        "    all_paths[split_name] = paths\n",
        "    print(f\"{len(paths)} images with annotations created\")\n",
        "\n",
        "print(f\"\\nTotal: {sum(len(p) for p in all_paths.values())} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vifXNOAP1pOR"
      },
      "outputs": [],
      "source": [
        "# Create directory for YOLO config files\n",
        "CONFIG_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Write train.txt, val.txt, test.txt\n",
        "for split_name, paths in all_paths.items():\n",
        "    txt_file = CONFIG_PATH / f\"{split_name}.txt\"\n",
        "    with open(txt_file, 'w') as f:\n",
        "        f.write('\\n'.join(paths))\n",
        "    print(f\"Created: {txt_file} ({len(paths)} images)\")\n",
        "\n",
        "# Write classes.names file\n",
        "names_file = CONFIG_PATH / \"classes.names\"\n",
        "with open(names_file, 'w') as f:\n",
        "    f.write(\"person\\n\")\n",
        "print(f\"Created: {names_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taYyYTldU4Ty"
      },
      "source": [
        "## Prepare Model\n",
        "\n",
        "The original Yolo-Fastest was built using the [Darknet](https://github.com/pjreddie/darknet) framework. We need to download the Yolo-Fastest repo, convert the model to PyTorch, and load the preexisting weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-c0OJb3S8vx"
      },
      "outputs": [],
      "source": [
        "# Download the model repo (we need the model architecture and weights)\n",
        "!curl -L -o {REPO_ZIP_PATH} {REPO_URL}\n",
        "\n",
        "# Extract and rename to consistent path\n",
        "with zipfile.ZipFile(REPO_ZIP_PATH, 'r') as zip_ref:\n",
        "    # Get top-level folder name from zip contents\n",
        "    top_folder = zip_ref.namelist()[0].split('/')[0]\n",
        "    zip_ref.extractall(REPO_PATH.parent)\n",
        "\n",
        "# Rename extracted folder to desired name\n",
        "extracted_path = REPO_PATH.parent / top_folder\n",
        "extracted_path.rename(REPO_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjHKHeVW4M1e"
      },
      "outputs": [],
      "source": [
        "# Read the original config\n",
        "with open(MODEL_CFG_PATH, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Change input size strings in config\n",
        "new_content = content.replace(\"width=320\", f\"width={IMG_WIDTH}\")\n",
        "new_content = new_content.replace(\"height=320\", f\"height={IMG_HEIGHT}\")\n",
        "\n",
        "# Save modified config\n",
        "with open(MODIFIED_CFG_PATH, 'w') as f:\n",
        "    f.write(new_content)\n",
        "\n",
        "# Verify changes\n",
        "with open(MODIFIED_CFG_PATH, 'r') as f:\n",
        "    first_lines = ''.join(f.readlines()[:15])\n",
        "print(f\"First lines of modified config:\\n{first_lines}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUWY4ITkC9Qg"
      },
      "outputs": [],
      "source": [
        "# Download converter script\n",
        "urllib.request.urlretrieve(CONVERTER_SCRIPT_URL, CONVERTER_SCRIPT_PATH)\n",
        "print(f\"Downloaded converter script to {CONVERTER_SCRIPT_PATH}\")\n",
        "\n",
        "# Import classes and functions from our converter script\n",
        "from yolo_fastest_to_pytorch import YoloFastest, ConvNoBN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aue4I6LdBOjT"
      },
      "outputs": [],
      "source": [
        "# Build model\n",
        "model = YoloFastest(str(MODIFIED_CFG_PATH), input_size=IMG_HEIGHT)\n",
        "print(f\"Model built! Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Load weights\n",
        "model.load_darknet_weights(str(MODEL_WEIGHTS_PATH))\n",
        "\n",
        "# Test forward pass\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    dummy = torch.randn(1, 3, IMG_HEIGHT, IMG_WIDTH)\n",
        "    outputs = model(dummy)\n",
        "    print(f\"Output shapes: {[o.shape for o in outputs]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdLO8uWZG4UL"
      },
      "source": [
        "## Replace Detection Heads\n",
        "\n",
        "We need to replace the detection heads (final layers that predict one of 80 COCO classes by default) with custom layers that predict our 1 class (\"person\").\n",
        "\n",
        "YOLO-Fastest has 2 detection heads that output predictions on two different grids (a coarse 6x6 grid and a finer 12x12 grid). Each grid cell contains 3 anchor points where an object might appear (anchors are predefined bounding boxes with different aspect ratios). The output of each detection head is a tensor with shape `[batch, channels, grid_height, grid_width]` where:\n",
        "\n",
        "* `batch`: number of images. For inference, this will be 1.\n",
        "* `channels`: 18 values representing 3 anchors with 6 predictions per anchor:\n",
        "  * `tx, ty`: box center offset (relative to grid cell)\n",
        "  * `tw, th`: box width/height scaling (relative to anchor size)\n",
        "  * `obj`: objectness score (confidence that an object exists)\n",
        "  * `cls`: class probability (\"person\")\n",
        "* `grid_height`: number of rows of cells\n",
        "* `grid_width`: number of columns of cells\n",
        "\n",
        "The original COCO detection heads output 255 channels (3 anchors × 85 values, where 85 = 4 box + 1 obj + 80 classes). We replace these with heads that output 18 channels (3 anchors × 6 values, where 6 = 4 box + 1 obj + 1 class).\n",
        "\n",
        "We should expect the output of the coarse detection head to have the shape `[1, 18, 6, 6]` and the output of the finer detection head to have the shape `[1, 18, 12, 12]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HeeZkUUBPGX"
      },
      "outputs": [],
      "source": [
        "def find_detection_heads(model):\n",
        "    \"\"\"Find indices of conv layers immediately before yolo layers.\"\"\"\n",
        "    head_indices = []\n",
        "    layer_idx = 0\n",
        "\n",
        "    for i, block in enumerate(model.blocks):\n",
        "        if block['type'] == 'net':\n",
        "            continue\n",
        "\n",
        "        if block['type'] == 'yolo':\n",
        "            # The conv layer before this yolo is a detection head\n",
        "            # Search backwards for the most recent conv\n",
        "            search_idx = layer_idx - 1\n",
        "            while search_idx >= 0:\n",
        "                # Check the block type at this index\n",
        "                block_idx = 0\n",
        "                for b in model.blocks:\n",
        "                    if b['type'] == 'net':\n",
        "                        continue\n",
        "                    if block_idx == search_idx:\n",
        "                        if b['type'] == 'convolutional':\n",
        "                            head_indices.append(search_idx)\n",
        "                        break\n",
        "                    block_idx += 1\n",
        "                break\n",
        "\n",
        "        layer_idx += 1\n",
        "\n",
        "    return head_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dD2qKDSFZah"
      },
      "outputs": [],
      "source": [
        "# Find detection head layers programmatically\n",
        "head_indices = find_detection_heads(model)\n",
        "print(f\"Detection head layer indices: {head_indices}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLr7TsMnF0La"
      },
      "outputs": [],
      "source": [
        "# Replace detection heads with our 1-class detection heads\n",
        "for idx in head_indices:\n",
        "    old_conv = model.module_list[idx].conv\n",
        "    model.module_list[idx] = ConvNoBN(\n",
        "        in_ch=old_conv.in_channels,\n",
        "        out_ch=NEW_FILTERS,\n",
        "        kernel=old_conv.kernel_size[0],\n",
        "        stride=old_conv.stride[0],\n",
        "        pad=old_conv.padding[0],\n",
        "        activation='linear'\n",
        "    )\n",
        "\n",
        "# Verify new output shapes\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    dummy = torch.randn(1, 3, IMG_HEIGHT, IMG_WIDTH)\n",
        "    outputs = model(dummy)\n",
        "    print(f\"New output shapes: {[o.shape for o in outputs]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgZstHGAGGTU"
      },
      "outputs": [],
      "source": [
        "# Move model to device\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3j9lpbUT_PF"
      },
      "source": [
        "## Freeze Parts of the Model\n",
        "\n",
        "You have the option to *freeze* sections of the model. Freezing preserves the pretrained feature extraction capabilities, preventing the model from \"forgetting\" what it learned on COCO. This is useful when your dataset is small (risk of overfitting) or very similar to the original training data (features already work well). Freezing also speeds up training since fewer gradients need to be computed. You have the option of freezing either:\n",
        "\n",
        "* **Backbone:** Extracts basic visual features from the image (e.g., edges, textures, shapes, object parts). YOLO-Fastest uses depthwise separable convolutions and residual blocks for efficient feature extraction. These low-level features are generally universal across datasets.\n",
        "* **Neck:** Fuses features from different scales to help detect both large and small objects. This includes:\n",
        "  * *Spatial Pyramid Pooling (SPP)*: Captures multi-scale context using parallel pooling operations at different kernel sizes\n",
        "  * *FPN-like layers*: Upsamples and concatenates features from earlier backbone layers, combining semantic (what) and spatial (where) information\n",
        "\n",
        "Note that the **detection heads** are single convolutional layers that output predictions for each grid cell. Each prediction contains box coordinates, objectness score, and class probability — all combined into 18 channels (3 anchors × 6 values). We replaced these heads for our single class, and they will always remain trainable.\n",
        "\n",
        "A good starting point is to freeze both backbone and neck, training only the detection heads. If performance is insufficient, try unfreezing the neck to allow the feature fusion layers to adapt to your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77AgxLe3TzO_"
      },
      "outputs": [],
      "source": [
        "def freeze_model_layers(model, freeze_backbone=True, freeze_neck=True):\n",
        "    \"\"\"\n",
        "    Freeze portions of the YOLO-Fastest model. Note that the layers are hardcoded.\n",
        "\n",
        "    Architecture breakdown:\n",
        "    - Backbone: layers 0-108 (feature extraction)\n",
        "    - Neck/SPP: layers 109-119, 122-128 (feature fusion)\n",
        "    - Detection heads: layers 120, 129 (class-specific, always trainable)\n",
        "    \"\"\"\n",
        "    # Find detection head indices (layers with output matching our NEW_FILTERS)\n",
        "    head_indices = set()\n",
        "    for i, module in enumerate(model.module_list):\n",
        "        if hasattr(module, 'conv') and module.conv.out_channels == NEW_FILTERS:\n",
        "            head_indices.add(i)\n",
        "\n",
        "    # Define layer ranges\n",
        "    backbone_range = range(0, 109)  # Layers 0-108\n",
        "    neck_range = list(range(109, 120)) + list(range(122, 129))  # Exclude heads\n",
        "\n",
        "    # Freeze backbone\n",
        "    if freeze_backbone:\n",
        "        for i in backbone_range:\n",
        "            for param in model.module_list[i].parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Freeze neck\n",
        "    if freeze_neck:\n",
        "        for i in neck_range:\n",
        "            for param in model.module_list[i].parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Ensure detection heads are always trainable\n",
        "    for i in head_indices:\n",
        "        for param in model.module_list[i].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    return head_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvv5TA61UzVZ"
      },
      "outputs": [],
      "source": [
        "# Apply freezing\n",
        "head_indices = freeze_model_layers(model, FREEZE_BACKBONE, FREEZE_NECK)\n",
        "print(f\"Backbone frozen: {FREEZE_BACKBONE}\")\n",
        "print(f\"Neck frozen: {FREEZE_NECK}\")\n",
        "\n",
        "# Count trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Parameters: {trainable_params:,} trainable / {total_params:,} total\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbRcsWc7KKJO"
      },
      "source": [
        "## Create DataLoaders\n",
        "\n",
        "We need to create loaders for our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elc86JepKJmg"
      },
      "outputs": [],
      "source": [
        "class YoloDataset(Dataset):\n",
        "    \"\"\"YOLO-format dataset loader using image list file.\"\"\"\n",
        "\n",
        "    def __init__(self, list_file, img_size=192, augment=False):\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "\n",
        "        # Read image from file\n",
        "        with open(list_file, 'r') as f:\n",
        "            self.img_files = [Path(line.strip()) for line in f if line.strip()]\n",
        "\n",
        "        print(f\"Loaded {len(self.img_files)} images from {list_file}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = self.img_files[idx]\n",
        "        img = cv2.imread(str(img_path))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        orig_h, orig_w = img.shape[:2]\n",
        "\n",
        "        # Load labels\n",
        "        label_path = img_path.with_suffix('.txt')\n",
        "        if label_path.exists() and label_path.stat().st_size > 0:\n",
        "            labels = np.loadtxt(str(label_path)).reshape(-1, 5).copy()\n",
        "        else:\n",
        "            labels = np.zeros((0, 5))\n",
        "\n",
        "        # Apply augmentation before resize\n",
        "        if self.augment and len(labels) > 0:\n",
        "            img, labels = self.apply_augmentation(img, labels)\n",
        "\n",
        "        # Resize\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "\n",
        "        # Normalize to [0, 1] and convert to CHW\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        img = torch.from_numpy(img).permute(2, 0, 1)\n",
        "\n",
        "        if len(labels) > 0:\n",
        "            labels = torch.from_numpy(labels).float()\n",
        "        else:\n",
        "            labels = torch.zeros((0, 5))\n",
        "\n",
        "        return img, labels\n",
        "\n",
        "    def apply_augmentation(self, img, labels):\n",
        "        \"\"\"Apply random augmentations.\"\"\"\n",
        "        # Random horizontal flip (50% chance)\n",
        "        if random.random() > 0.5:\n",
        "            img = cv2.flip(img, 1)\n",
        "            labels[:, 1] = 1.0 - labels[:, 1]  # Flip x_center\n",
        "\n",
        "        # Random brightness adjustment\n",
        "        if random.random() > 0.5:\n",
        "            factor = 0.7 + random.random() * 0.6  # 0.7 to 1.3\n",
        "            img = np.clip(img * factor, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # Random contrast adjustment\n",
        "        if random.random() > 0.5:\n",
        "            factor = 0.7 + random.random() * 0.6  # 0.7 to 1.3\n",
        "            mean = img.mean()\n",
        "            img = np.clip((img - mean) * factor + mean, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # Random saturation adjustment\n",
        "        if random.random() > 0.5:\n",
        "            img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
        "            factor = 0.7 + random.random() * 0.6  # 0.7 to 1.3\n",
        "            img_hsv[:, :, 1] = np.clip(img_hsv[:, :, 1] * factor, 0, 255)\n",
        "            img = cv2.cvtColor(img_hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
        "\n",
        "        return img, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwnYcKJfLAxY"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for variable-length labels.\"\"\"\n",
        "    imgs, labels = zip(*batch)\n",
        "\n",
        "    # Stack images\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "    # Add batch index to labels and concatenate\n",
        "    batch_labels = []\n",
        "    for i, label in enumerate(labels):\n",
        "        if len(label) > 0:\n",
        "            # Add batch index as first column\n",
        "            batch_idx = torch.full((len(label), 1), i)\n",
        "            batch_labels.append(torch.cat([batch_idx, label], dim=1))\n",
        "\n",
        "    if batch_labels:\n",
        "        labels = torch.cat(batch_labels, dim=0)  # [N, 6]: batch_idx, class, x, y, w, h\n",
        "    else:\n",
        "        labels = torch.zeros((0, 6))\n",
        "\n",
        "    return imgs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa30V4jRLcRG"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = YoloDataset(CONFIG_PATH / 'train.txt', img_size=IMG_HEIGHT, augment=True)\n",
        "val_dataset = YoloDataset(CONFIG_PATH / 'val.txt', img_size=IMG_HEIGHT, augment=False)\n",
        "test_dataset = YoloDataset(CONFIG_PATH / 'test.txt', img_size=IMG_HEIGHT, augment=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XsxqKwaN0mt"
      },
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if device.type == 'cuda' else False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_loader)} batches\")\n",
        "print(f\"Val: {len(val_loader)} batches\")\n",
        "print(f\"Test: {len(test_loader)} batches\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0GznsuZOFGB"
      },
      "outputs": [],
      "source": [
        "# Get one batch and verify details\n",
        "imgs, labels = next(iter(train_loader))\n",
        "print(f\"Images: {imgs.shape}\")\n",
        "print(f\"Labels: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW_fGgP4OWRi"
      },
      "outputs": [],
      "source": [
        "# Pick an index in the batch\n",
        "idx = 3\n",
        "\n",
        "# Get an image and its labels\n",
        "img = imgs[idx].permute(1, 2, 0).numpy()  # CHW -> HWC, range [0, 1]\n",
        "img_labels = labels[labels[:, 0] == idx]\n",
        "\n",
        "# Create plots\n",
        "fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "\n",
        "# Option 1: Display float image directly (matplotlib handles [0, 1] range)\n",
        "ax.imshow(img)\n",
        "\n",
        "# Option 2: Convert to uint8 properly\n",
        "# ax.imshow((img * 255).astype('uint8'))\n",
        "\n",
        "# Draw bounding boxes\n",
        "for label in img_labels:\n",
        "    # Calculate box coordinates\n",
        "    _, cls, cx, cy, w, h = label\n",
        "    x = (cx - w/2) * IMG_WIDTH\n",
        "    y = (cy - h/2) * IMG_HEIGHT\n",
        "    width = w * IMG_WIDTH\n",
        "    height = h * IMG_HEIGHT\n",
        "\n",
        "    # Add green rectangle for bounding box\n",
        "    rect = patches.Rectangle(\n",
        "        (x, y),\n",
        "        width,\n",
        "        height,\n",
        "        linewidth=2,\n",
        "        edgecolor='lime',\n",
        "        facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "# Show image info\n",
        "ax.set_title(f\"Sample image with {len(img_labels)} bounding boxes\")\n",
        "ax.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-1Wn6I-WGgR"
      },
      "source": [
        "## Define Loss Function\n",
        "\n",
        "We will use some helper functions from the custom [yolo_fastest_utils.py](https://github.com/ShawnHymel/intro-to-edge-ai-and-cv-with-renesas/blob/main/scripts/yolo_fastest_utils.py) script along with the custom loss function defined in [yolo_fastest_loss.py](https://github.com/ShawnHymel/intro-to-edge-ai-and-cv-with-renesas/blob/main/scripts/yolo_fastest_loss.py).\n",
        "\n",
        "For each ground truth box, we find which grid cell contains its center and which anchor best matches its shape (based on IoU of width/height). We then compute three losses: complete IoU (CIoU) loss for box coordinates (penalizes poor overlap, center distance, and aspect ratio mismatch), binary cross-entropy (BCE) loss for objectness (does this anchor contain an object?), and BCE loss for classification (is it a person?). The losses are weighted and summed across both output scales (6×6 and 12×12 grids)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSZ0DKD_WGQ4"
      },
      "outputs": [],
      "source": [
        "# Download utilities script\n",
        "urllib.request.urlretrieve(UTILS_SCRIPT_URL, UTILS_SCRIPT_PATH)\n",
        "print(f\"Downloaded utils script to {UTILS_SCRIPT_PATH}\")\n",
        "\n",
        "# Import utilities\n",
        "from yolo_fastest_utils import (\n",
        "    get_anchors_from_config,\n",
        "    box_iou,\n",
        "    bbox_iou_tensor,\n",
        "    box_iou_wh,\n",
        "    build_targets,\n",
        "    decode_predictions,\n",
        "    nms,\n",
        "    compute_map\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXFy33cVirNH"
      },
      "outputs": [],
      "source": [
        "# Download loss calculation class\n",
        "urllib.request.urlretrieve(LOSS_SCRIPT_URL, LOSS_SCRIPT_PATH)\n",
        "print(f\"Downloaded loss script to {LOSS_SCRIPT_PATH}\")\n",
        "\n",
        "# Import loss function\n",
        "from yolo_fastest_loss import YoloLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fhCbDVkOcct"
      },
      "outputs": [],
      "source": [
        "# Get anchors from model config\n",
        "anchors, anchor_masks = get_anchors_from_config(model)\n",
        "print(f\"Anchors: {anchors}\")\n",
        "print(f\"Masks: {anchor_masks}\")\n",
        "\n",
        "# Create loss function\n",
        "criterion = YoloLoss(\n",
        "    anchors=anchors,\n",
        "    anchor_masks=anchor_masks,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    img_size=IMG_HEIGHT\n",
        ")\n",
        "print(f\"Grid sizes: {criterion.grid_sizes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bZETOszhfwr"
      },
      "outputs": [],
      "source": [
        "# Get a set of images and labels from the training dataset\n",
        "model.train()\n",
        "imgs, labels = next(iter(train_loader))\n",
        "imgs = imgs.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "# Calculate losses using our custom loss function\n",
        "outputs = model(imgs)\n",
        "loss, box_loss, obj_loss, cls_loss = criterion(outputs, labels)\n",
        "\n",
        "# Print losses\n",
        "print(f\"Box:   {box_loss.item():.4f}\")\n",
        "print(f\"Obj:   {obj_loss.item():.4f}\")\n",
        "print(f\"Cls:   {cls_loss.item():.4f}\")\n",
        "print(f\"Total: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z60w74vjkoDf"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "We are now ready to train our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-HZ4PY1i9oL"
      },
      "outputs": [],
      "source": [
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ifUXf68j1Pf"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model,\n",
        "    train_loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    device,\n",
        "    verbose=False\n",
        "):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    total_loss = 0\n",
        "    total_box_loss = 0\n",
        "    total_obj_loss = 0\n",
        "    total_cls_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    # Enable training-specific behaviors (e.g. dropout)\n",
        "    model.train()\n",
        "\n",
        "    # Do one full training cycle on a batch of training data\n",
        "    for i, (imgs, labels) in enumerate(train_loader):\n",
        "        # Move data to the same device as the model\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(imgs)\n",
        "        loss, box_loss, obj_loss, cls_loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        total_loss += loss.item()\n",
        "        total_box_loss += box_loss.item()\n",
        "        total_obj_loss += obj_loss.item()\n",
        "        total_cls_loss += cls_loss.item()\n",
        "\n",
        "        # Optionally print progress every 50 batches\n",
        "        if verbose and (i+1) % 50 == 0:\n",
        "            print(f\"    Batch {i+1}/{num_batches} - loss: {loss.item():.2f}\")\n",
        "\n",
        "    # Calculate averages\n",
        "    return {\n",
        "        'loss': total_loss / num_batches,\n",
        "        'box': total_box_loss / num_batches,\n",
        "        'obj': total_obj_loss / num_batches,\n",
        "        'cls': total_cls_loss / num_batches\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q8IGNoflPuX"
      },
      "outputs": [],
      "source": [
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    total_loss = 0\n",
        "    total_box_loss = 0\n",
        "    total_obj_loss = 0\n",
        "    total_cls_loss = 0\n",
        "    num_batches = len(val_loader)\n",
        "\n",
        "    # Disable training-specific behaviors (e.g. dropout)\n",
        "    model.eval()\n",
        "\n",
        "    # Do not track gradients during validation\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            # Move data to the same device as the model\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(imgs)\n",
        "            loss, box_loss, obj_loss, cls_loss = criterion(outputs, labels)\n",
        "\n",
        "            # Get total loss for the batch\n",
        "            total_loss += loss.item()\n",
        "            total_box_loss += box_loss.item()\n",
        "            total_obj_loss += obj_loss.item()\n",
        "            total_cls_loss += cls_loss.item()\n",
        "\n",
        "    # Calculate averages\n",
        "    return {\n",
        "        'loss': total_loss / num_batches,\n",
        "        'box': total_box_loss / num_batches,\n",
        "        'obj': total_obj_loss / num_batches,\n",
        "        'cls': total_cls_loss / num_batches\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMjZTi37lRXJ"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "history = {\n",
        "    'train_loss': [], 'train_box': [], 'train_obj': [], 'train_cls': [],\n",
        "    'val_loss': [], 'val_box': [], 'val_obj': [], 'val_cls': []\n",
        "}\n",
        "\n",
        "# Train for a number of epochs, save best model (lowest val loss)\n",
        "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    # Train\n",
        "    train_metrics = train_one_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        device,\n",
        "        True\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    val_metrics = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    # Record history\n",
        "    history['train_loss'].append(train_metrics['loss'])\n",
        "    history['train_box'].append(train_metrics['box'])\n",
        "    history['train_obj'].append(train_metrics['obj'])\n",
        "    history['train_cls'].append(train_metrics['cls'])\n",
        "    history['val_loss'].append(val_metrics['loss'])\n",
        "    history['val_box'].append(val_metrics['box'])\n",
        "    history['val_obj'].append(val_metrics['obj'])\n",
        "    history['val_cls'].append(val_metrics['cls'])\n",
        "\n",
        "    # Save best model\n",
        "    if val_metrics['loss'] < best_val_loss:\n",
        "        best_val_loss = val_metrics['loss']\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - \"\n",
        "              f\"Train: {train_metrics['loss']:.4f} - \"\n",
        "              f\"Val: {val_metrics['loss']:.4f} - \"\n",
        "              f\"Saved best model\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - \"\n",
        "              f\"Train: {train_metrics['loss']:.4f} - \"\n",
        "              f\"Val: {val_metrics['loss']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training complete!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL1wemP6nrdI"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Total loss\n",
        "axes[0].plot(history['train_loss'], label='Train')\n",
        "axes[0].plot(history['val_loss'], label='Validation')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Total Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Component losses\n",
        "axes[1].plot(history['train_box'], label='Train Box')\n",
        "axes[1].plot(history['val_box'], label='Val Box')\n",
        "axes[1].plot(history['train_obj'], label='Train Obj')\n",
        "axes[1].plot(history['val_obj'], label='Val Obj')\n",
        "axes[1].plot(history['train_cls'], label='Train Cls')\n",
        "axes[1].plot(history['val_cls'], label='Val Cls')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Loss Components')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1E81LhFGYNR"
      },
      "source": [
        "## Evaluate Model\n",
        "\n",
        "We will compute the mean average precision at 50% IoU (mAP@0.5) using our test set. To do that, we need write a few helper functions for computing the non-maximum suppression (NMS) and intersection over union (IoU).\n",
        "\n",
        "We'll also see how the model performs on a single image by drawing the ground-truth bounding boxes and comparing them to the predicted bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD5jm_H1GDs5"
      },
      "outputs": [],
      "source": [
        "# Load the best model weights and switch to evaluation mode\n",
        "model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
        "model.eval()\n",
        "print(f\"Loaded model from {BEST_MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRaeJb9FGa3S"
      },
      "outputs": [],
      "source": [
        "# Compute mAP on test set\n",
        "test_map, total_preds, total_gt = compute_map(\n",
        "    model=model,\n",
        "    data_loader=test_loader,\n",
        "    anchors=anchors,\n",
        "    anchor_masks=anchor_masks,\n",
        "    img_size=IMG_HEIGHT,\n",
        "    device=device,\n",
        "    conf_thresh=0.25,\n",
        "    iou_thresh=0.5\n",
        ")\n",
        "\n",
        "print(f\"Test mAP@0.5: {test_map:.4f}\")\n",
        "print(f\"Total predictions: {total_preds}\")\n",
        "print(f\"Total ground truth: {total_gt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG8HFfAHGfwB"
      },
      "outputs": [],
      "source": [
        "# Choose an index into the test dataset\n",
        "idx = 42\n",
        "\n",
        "# Get a single test image directly from dataset\n",
        "img, label = test_dataset[idx]\n",
        "\n",
        "# Prepare image for display (already RGB, values in [0, 1])\n",
        "img_np = img.permute(1, 2, 0).numpy()\n",
        "\n",
        "# Run inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    img_input = img.unsqueeze(0).float().to(device)  # Add batch dimension\n",
        "    outputs = model(img_input)\n",
        "\n",
        "# Decode predictions and apply NMS (updated signature)\n",
        "predictions = decode_predictions(\n",
        "    outputs,\n",
        "    anchors=anchors,\n",
        "    anchor_masks=anchor_masks,\n",
        "    img_size=IMG_HEIGHT,\n",
        "    conf_thresh=0.25\n",
        ")\n",
        "pred_boxes = nms(predictions[0], iou_thresh=0.45)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
        "ax.imshow(img_np)  # matplotlib handles [0, 1] float range\n",
        "\n",
        "# Draw ground truth boxes (green)\n",
        "# Note: label shape is [N, 5] with (class, cx, cy, w, h) - no batch_idx\n",
        "for lbl in label:\n",
        "    cx = lbl[1].item() * IMG_WIDTH\n",
        "    cy = lbl[2].item() * IMG_HEIGHT\n",
        "    w = lbl[3].item() * IMG_WIDTH\n",
        "    h = lbl[4].item() * IMG_HEIGHT\n",
        "    x = cx - w / 2\n",
        "    y = cy - h / 2\n",
        "    rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='lime', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "# Draw predicted boxes (red)\n",
        "for pred in pred_boxes:\n",
        "    x1, y1, x2, y2, conf, cls = pred\n",
        "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(x1, y1-5, f'{conf:.2f}', color='red', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.set_title(f\"Green: Ground Truth, Red: Predictions ({len(pred_boxes)} detections)\")\n",
        "ax.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Ground truth boxes: {len(label)}\")\n",
        "print(f\"Predicted boxes: {len(pred_boxes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSdmnJJwHvGF"
      },
      "source": [
        "## Export Model\n",
        "\n",
        "Note that in most cases, the export process will produce 2 separate files:\n",
        "* **.onnx** - Model architecture and metadata (with references to external weight data)\n",
        "* **.onnx.data** - Model weights (external data file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfndCDVTHIUa"
      },
      "outputs": [],
      "source": [
        "# Put the model into evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create a dummy input tensor with the same shape as one sample (batch=1)\n",
        "dummy_input = torch.randn(1, 3, IMG_HEIGHT, IMG_WIDTH).to(device)\n",
        "\n",
        "# Export to ONNX\n",
        "torch.onnx.export(\n",
        "    model,                              # Model to export\n",
        "    dummy_input,                        # Example input (for tracing)\n",
        "    ONNX_PATH,                          # Output file path\n",
        "    export_params=True,                 # Export with trained weights\n",
        "    opset_version=ONNX_OPSET_VERSION,   # Which operations are supported\n",
        "    do_constant_folding=True,           # Optimize constant operations\n",
        "    input_names=['input'],              # Name for input layer\n",
        "    output_names=['output'],            # Name for output layer\n",
        "    dynamic_axes=None                   # Fixed batch size of 1\n",
        ")\n",
        "print(f\"Model exported to: {ONNX_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Y7Di50Fk0"
      },
      "source": [
        "## Export Calibration Data\n",
        "\n",
        "Export a few samples from the validation set to act as calibration data for post-training quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM4YaoiVzLqB"
      },
      "outputs": [],
      "source": [
        "# Don't exceed the total number of available samples\n",
        "num_samples = min(NUM_CALIB_SAMPLES, len(val_dataset))\n",
        "\n",
        "# Randomly choose from validation set\n",
        "indices = random.sample(range(len(val_dataset)), num_samples)\n",
        "\n",
        "# Get samples (ignore the labels) and convert to NumPy arrays (float32 format)\n",
        "calib_samples = []\n",
        "for i in indices:\n",
        "    x, _ = val_dataset[i]\n",
        "    calib_samples.append(x.float().numpy())\n",
        "\n",
        "# Stack into a single array: shape (num_samples, 3, H, W)\n",
        "calib_data = np.stack(calib_samples, axis=0)\n",
        "\n",
        "# Save samples as NPZ\n",
        "np.savez(CALIB_NPZ_PATH, input=calib_data)\n",
        "print(f\"Calibration data shape: {calib_data.shape}\")\n",
        "print(f\"Calibration data dtype: {calib_data.dtype}\")\n",
        "print(f\"Calibration data range: [{calib_data.min():.1f}, {calib_data.max():.1f}]\")\n",
        "print(f\"Saved calibration data to: {CALIB_NPZ_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4K_Cv5A0KNK"
      },
      "source": [
        "## Deploy!\n",
        "\n",
        "Download your *model.onnx* and *model.onnx.data* files along with the *calibration_data.npz* file. Use your vendor's toolset (e.g. RUHMI) to quantize, compress, and compile the model for your target device."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
