{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Person Detection Training\n",
        "\n",
        "In this notebook, we are going to use transfer learning to update a pre-trained [Yolo-Fastest](https://github.com/dog-qiuqiu/Yolo-Fastest) model to identify people in an image. We will retrain the model using a preexisting [dataset from Roboflow](https://universe.roboflow.com/titulacin/person-detection-9a6mk).\n",
        "\n",
        "In Google Colab, select **File > Open notebook** then select the **Upload** tab. Select this file to open it in Colab.\n",
        "\n",
        "Press **shift + enter** to execute each cell in order. Make sure you stop and read each text section, as there are some manual steps you will need to perform (e.g. upload dataset).\n",
        "\n",
        "> **IMPORTANT!** We are downloading our dataset from [Roboflow](https://roboflow.com/). To do that, you will need to sign up for a free account, go to your account and get your API key. Update the `ROBOFLOW_API_KEY` value with your personal API key."
      ],
      "metadata": {
        "id": "_PBtFsbroE-Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2yAdGHQmlH-"
      },
      "outputs": [],
      "source": [
        "# Install specific versions of the packages\n",
        "!python3 -m pip install \\\n",
        "    opencv-python=='4.13.0.90' \\\n",
        "    matplotlib=='3.10.0' \\\n",
        "    numpy=='2.0.2' \\\n",
        "    onnxscript=='0.5.7' \\\n",
        "    pandas=='2.2.2' \\\n",
        "    Pillow=='11.3.0' \\\n",
        "    roboflow=='1.2.13' \\\n",
        "    torch=='2.9.0'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import standard libraries\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "import types\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# Import third-party libraries\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from roboflow import Roboflow\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "Z2rpPSBwoqfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the versions of the libraries\n",
        "print(f\"OpenCV version: {cv2.__version__}\")\n",
        "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Pillow version: {Image.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ],
      "metadata": {
        "id": "N88kZ_nao0lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT! Update this with your Roboflow API key\n",
        "ROBOFLOW_API_KEY = \"paMbiEJKN9RxCvMqX0ji\""
      ],
      "metadata": {
        "id": "VXIMIpPq3J0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General settings\n",
        "SEED = 42\n",
        "\n",
        "# Dataset settings\n",
        "ROBOFLOW_WORKSPACE = \"titulacin\"\n",
        "ROBOFLOW_PROJECT = \"person-detection-9a6mk\"\n",
        "ROBOFLOW_PROJECT_VERSION = 16\n",
        "ROBOFLOW_DATASET_TYPE = \"yolov5\"\n",
        "DATASET_PATH = Path(\"/content/dataset\")\n",
        "CONFIG_PATH = Path(\"/content/yolo_config\")\n",
        "\n",
        "# Class names (see Roboflow project for names)\n",
        "CLASS_NAMES = [\"person\"]\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "# Pre-trained model settings\n",
        "REPO_URL = \"https://github.com/dog-qiuqiu/Yolo-Fastest/archive/refs/tags/v.1.1.0.zip\"\n",
        "REPO_ZIP_PATH = Path(\"/content/Yolo-Fastest.zip\")\n",
        "REPO_PATH = Path(\"/content/Yolo-Fastest\")\n",
        "WEIGHTS_PATH = REPO_PATH\n",
        "MODEL_CFG_PATH = REPO_PATH / \"ModelZoo/yolo-fastest-1.1_coco/yolo-fastest-1.1.cfg\"\n",
        "MODEL_WEIGHTS_PATH = REPO_PATH / \"ModelZoo/yolo-fastest-1.1_coco/yolo-fastest-1.1.weights\"\n",
        "MODIFIED_CFG_PATH = Path(\"/content/yolo-fastest-1.1_192_no_dropout.cfg\")\n",
        "\n",
        "# Darknet to PyTorch converter script\n",
        "CONVERTER_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/intro-to-edge-ai-and-cv-with-renesas/main/scripts/yolo_fastest_to_pytorch.py\"\n",
        "CONVERTER_SCRIPT_PATH = Path(\"/content/yolo_fastest_to_pytorch.py\")\n",
        "\n",
        "# Utilities script\n",
        "UTILS_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/intro-to-edge-ai-and-cv-with-renesas/main/scripts/yolo_fastest_utils.py\"\n",
        "UTILS_SCRIPT_PATH = Path(\"/content/yolo_fastest_utils.py\")\n",
        "\n",
        "# Loss calculation script\n",
        "LOSS_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/intro-to-edge-ai-and-cv-with-renesas/main/scripts/yolo_fastest_loss.py\"\n",
        "LOSS_SCRIPT_PATH = Path(\"/content/yolo_fastest_loss.py\")\n",
        "\n",
        "# Image preprocessing settings\n",
        "IMG_WIDTH = 320\n",
        "IMG_HEIGHT = 320\n",
        "\n",
        "# Model settings\n",
        "NUM_ANCHORS = 3\n",
        "NUM_CLASSES = 1\n",
        "NEW_FILTERS = NUM_ANCHORS * (5 + NUM_CLASSES)  # 3 * 6 = 18\n",
        "\n",
        "# Model update settings\n",
        "NUM_CLASSES = 1\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2 # Number of parallel processes for loading data\n",
        "\n",
        "# Freeze settings (set to True to freeze)\n",
        "FREEZE_BACKBONE = False\n",
        "FREEZE_NECK = False\n",
        "\n",
        "# Training settings\n",
        "NUM_EPOCHS = 30\n",
        "LEARNING_RATE = 0.0001\n",
        "BEST_MODEL_PATH = Path(\"/content/best_model.pth\")\n",
        "\n",
        "# PyTorch export settings\n",
        "PYTORCH_PATH = Path(\"/content/model.pt\")\n",
        "\n",
        "# ONNX export settings\n",
        "ONNX_OPSET_VERSION = 18\n",
        "ONNX_PATH = Path(\"/content/model.onnx\")\n",
        "\n",
        "# Calibration data settings\n",
        "NUM_CALIB_SAMPLES = 20\n",
        "CALIB_NPZ_PATH = Path(\"/content/calibration_data.npz\")\n",
        "\n",
        "# LiteRT (tflite) export settings\n",
        "TFLITE_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/intro-to-edge-ai-and-cv-with-renesas/main/scripts/yolo_fastest_to_tflite.py\"\n",
        "TFLITE_SCRIPT_PATH = Path(\"/content/yolo_fastest_to_tflite.py\")\n",
        "TFLITE_PATH = Path(\"/content/model.tflite\")\n",
        "\n",
        "# Test sample settings\n",
        "TEST_SAMPLE_NPZ_PATH = Path(\"/content/test_sample.npz\")\n",
        "TEST_SAMPLE_H_PATH = Path(\"/content/test_sample.h\")"
      ],
      "metadata": {
        "id": "dltaJwHBo195"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "lsM8qVjgZ5xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target compute device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "PiJIRY9UZ6iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset\n",
        "\n",
        "Rather than create our own (which is very time consuming), we will use an existing dataset to retrain the model.\n",
        "\n",
        "We first download and unzip the dataset. After that, we covert the annotations to YOLO-style (normalized bounding box information in a .txt file for each image). Then, we create a few text files in *yolo_config/* that lists the classes and gives the location of each image file."
      ],
      "metadata": {
        "id": "Jxfxp3h_pnT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset from Roboflow\n",
        "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "project = rf.workspace(ROBOFLOW_WORKSPACE).project(ROBOFLOW_PROJECT)\n",
        "version = project.version(ROBOFLOW_PROJECT_VERSION)\n",
        "dataset = version.download(ROBOFLOW_DATASET_TYPE, location=str(DATASET_PATH))"
      ],
      "metadata": {
        "id": "AGgjIUBAppNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build image path lists directly from the Roboflow directory structure\n",
        "CONFIG_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define dataset splits\n",
        "splits = {'train': 'train', 'val': 'valid', 'test': 'test'}\n",
        "all_paths = {}\n",
        "\n",
        "# Construct YOLO config files\n",
        "for split_name, folder_name in splits.items():\n",
        "    img_dir = DATASET_PATH / folder_name / 'images'\n",
        "    paths = sorted([str(p) for p in img_dir.iterdir()\n",
        "                    if p.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']])\n",
        "    all_paths[split_name] = paths\n",
        "\n",
        "    # Write list file\n",
        "    txt_file = CONFIG_PATH / f\"{split_name}.txt\"\n",
        "    with open(txt_file, 'w') as f:\n",
        "        f.write('\\n'.join(paths))\n",
        "    print(f\"Created: {txt_file} ({len(paths)} images)\")\n",
        "\n",
        "# Write classes.names\n",
        "names_file = CONFIG_PATH / \"classes.names\"\n",
        "with open(names_file, 'w') as f:\n",
        "    for name in CLASS_NAMES:\n",
        "        f.write(f\"{name}\\n\")\n",
        "print(f\"Created: {names_file} ({len(CLASS_NAMES)} classes)\")"
      ],
      "metadata": {
        "id": "1JHRhBhQ3bl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create DataLoaders\n",
        "\n",
        "We need to create loaders for our dataset."
      ],
      "metadata": {
        "id": "EbRcsWc7KKJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YoloDataset(Dataset):\n",
        "    \"\"\"YOLO-format dataset loader using image list file.\"\"\"\n",
        "\n",
        "    def __init__(self, list_file, img_size=192, augment=False):\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "\n",
        "        # Read image from file\n",
        "        with open(list_file, 'r') as f:\n",
        "            self.img_files = [Path(line.strip()) for line in f if line.strip()]\n",
        "\n",
        "        print(f\"Loaded {len(self.img_files)} images from {list_file}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = self.img_files[idx]\n",
        "        img = cv2.imread(str(img_path))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        orig_h, orig_w = img.shape[:2]\n",
        "\n",
        "        # Load labels (labels are in a parallel 'labels/' folder)\n",
        "        label_path = img_path.parent.parent / 'labels' / img_path.with_suffix('.txt').name\n",
        "        if label_path.exists() and label_path.stat().st_size > 0:\n",
        "            labels = np.loadtxt(str(label_path)).reshape(-1, 5).copy()\n",
        "        else:\n",
        "            labels = np.zeros((0, 5))\n",
        "\n",
        "        # Apply augmentation before crop/resize\n",
        "        if self.augment and len(labels) > 0:\n",
        "            img, labels = self.apply_augmentation(img, labels)\n",
        "\n",
        "        # Center crop to square and adjust labels\n",
        "        img, labels = self.center_crop(img, labels)\n",
        "\n",
        "        # Resize cropped square to target size\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "\n",
        "        # Normalize to [0, 1] and convert to CHW\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        img = torch.from_numpy(img).permute(2, 0, 1)\n",
        "\n",
        "        if len(labels) > 0:\n",
        "            labels = torch.from_numpy(labels).float()\n",
        "        else:\n",
        "            labels = torch.zeros((0, 5))\n",
        "\n",
        "        return img, labels\n",
        "\n",
        "    def center_crop(self, img, labels):\n",
        "        \"\"\"Center crop image to square and adjust bounding boxes.\"\"\"\n",
        "        h, w = img.shape[:2]\n",
        "        crop_size = min(h, w)\n",
        "        x_offset = (w - crop_size) // 2\n",
        "        y_offset = (h - crop_size) // 2\n",
        "\n",
        "        # Crop image\n",
        "        img = img[y_offset:y_offset + crop_size, x_offset:x_offset + crop_size]\n",
        "\n",
        "        if len(labels) == 0:\n",
        "            return img, labels\n",
        "\n",
        "        # Convert normalized coords to pixel coords\n",
        "        cx = labels[:, 1] * w\n",
        "        cy = labels[:, 2] * h\n",
        "        bw = labels[:, 3] * w\n",
        "        bh = labels[:, 4] * h\n",
        "\n",
        "        # Shift to crop-relative pixel coords\n",
        "        cx -= x_offset\n",
        "        cy -= y_offset\n",
        "\n",
        "        # Convert to x1, y1, x2, y2 for clipping\n",
        "        x1 = cx - bw / 2\n",
        "        y1 = cy - bh / 2\n",
        "        x2 = cx + bw / 2\n",
        "        y2 = cy + bh / 2\n",
        "\n",
        "        # Clip to crop boundaries\n",
        "        x1 = np.clip(x1, 0, crop_size)\n",
        "        y1 = np.clip(y1, 0, crop_size)\n",
        "        x2 = np.clip(x2, 0, crop_size)\n",
        "        y2 = np.clip(y2, 0, crop_size)\n",
        "\n",
        "        # Compute clipped box dimensions\n",
        "        clipped_w = x2 - x1\n",
        "        clipped_h = y2 - y1\n",
        "        orig_area = bw * bh\n",
        "\n",
        "        # Keep boxes that retain at least 30% of their original area\n",
        "        keep = (clipped_w > 0) & (clipped_h > 0)\n",
        "        keep = keep & ((clipped_w * clipped_h) / (orig_area + 1e-6) > 0.3)\n",
        "\n",
        "        if not np.any(keep):\n",
        "            return img, np.zeros((0, 5))\n",
        "\n",
        "        # Rebuild labels in normalized YOLO format relative to crop\n",
        "        new_cx = ((x1[keep] + x2[keep]) / 2) / crop_size\n",
        "        new_cy = ((y1[keep] + y2[keep]) / 2) / crop_size\n",
        "        new_w = clipped_w[keep] / crop_size\n",
        "        new_h = clipped_h[keep] / crop_size\n",
        "\n",
        "        new_labels = np.stack([labels[keep, 0], new_cx, new_cy, new_w, new_h], axis=1)\n",
        "\n",
        "        return img, new_labels\n",
        "\n",
        "    def apply_augmentation(self, img, labels):\n",
        "        \"\"\"Apply random augmentations.\"\"\"\n",
        "        # Random horizontal flip (50% chance)\n",
        "        if random.random() > 0.5:\n",
        "            img = cv2.flip(img, 1)\n",
        "            labels[:, 1] = 1.0 - labels[:, 1]  # Flip x_center\n",
        "\n",
        "        # Random brightness adjustment\n",
        "        if random.random() > 0.5:\n",
        "            factor = 0.7 + random.random() * 0.6  # 0.7 to 1.3\n",
        "            img = np.clip(img * factor, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # Random contrast adjustment\n",
        "        if random.random() > 0.5:\n",
        "            factor = 0.7 + random.random() * 0.6  # 0.7 to 1.3\n",
        "            mean = img.mean()\n",
        "            img = np.clip((img - mean) * factor + mean, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # Random saturation adjustment\n",
        "        if random.random() > 0.5:\n",
        "            img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
        "            img_hsv[:, :, 1] *= 0.7 + random.random() * 0.6\n",
        "            img_hsv[:, :, 1] = np.clip(img_hsv[:, :, 1], 0, 255)\n",
        "            img = cv2.cvtColor(img_hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
        "\n",
        "        return img, labels"
      ],
      "metadata": {
        "id": "elc86JepKJmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for variable-length labels.\"\"\"\n",
        "    imgs, labels = zip(*batch)\n",
        "\n",
        "    # Stack images\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "    # Add batch index to labels and concatenate\n",
        "    batch_labels = []\n",
        "    for i, label in enumerate(labels):\n",
        "        if len(label) > 0:\n",
        "            # Add batch index as first column\n",
        "            batch_idx = torch.full((len(label), 1), i)\n",
        "            batch_labels.append(torch.cat([batch_idx, label], dim=1))\n",
        "\n",
        "    if batch_labels:\n",
        "        labels = torch.cat(batch_labels, dim=0)  # [N, 6]: batch_idx, class, x, y, w, h\n",
        "    else:\n",
        "        labels = torch.zeros((0, 6))\n",
        "\n",
        "    return imgs, labels"
      ],
      "metadata": {
        "id": "JwnYcKJfLAxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = YoloDataset(CONFIG_PATH / 'train.txt', img_size=IMG_HEIGHT, augment=True)\n",
        "val_dataset = YoloDataset(CONFIG_PATH / 'val.txt', img_size=IMG_HEIGHT, augment=False)\n",
        "test_dataset = YoloDataset(CONFIG_PATH / 'test.txt', img_size=IMG_HEIGHT, augment=False)"
      ],
      "metadata": {
        "id": "Fa30V4jRLcRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if device.type == 'cuda' else False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_loader)} batches\")\n",
        "print(f\"Val: {len(val_loader)} batches\")\n",
        "print(f\"Test: {len(test_loader)} batches\")"
      ],
      "metadata": {
        "id": "6XsxqKwaN0mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch and verify details\n",
        "imgs, labels = next(iter(train_loader))\n",
        "print(f\"Images: {imgs.shape}\")\n",
        "print(f\"Labels: {labels.shape}\")"
      ],
      "metadata": {
        "id": "J0GznsuZOFGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick an index in the batch\n",
        "idx = 9\n",
        "\n",
        "# Get an image and its labels\n",
        "img = imgs[idx].permute(1, 2, 0).numpy()  # CHW -> HWC, range [0, 1]\n",
        "img_labels = labels[labels[:, 0] == idx]\n",
        "\n",
        "# Create plots\n",
        "fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "\n",
        "# Option 1: Display float image directly (matplotlib handles [0, 1] range)\n",
        "ax.imshow(img)\n",
        "\n",
        "# Option 2: Convert to uint8 properly\n",
        "# ax.imshow((img * 255).astype('uint8'))\n",
        "\n",
        "# Draw bounding boxes\n",
        "for label in img_labels:\n",
        "    # Calculate box coordinates\n",
        "    _, cls, cx, cy, w, h = label\n",
        "    x = (cx - w/2) * IMG_WIDTH\n",
        "    y = (cy - h/2) * IMG_HEIGHT\n",
        "    width = w * IMG_WIDTH\n",
        "    height = h * IMG_HEIGHT\n",
        "\n",
        "    # Add green rectangle for bounding box\n",
        "    rect = patches.Rectangle(\n",
        "        (x, y),\n",
        "        width,\n",
        "        height,\n",
        "        linewidth=2,\n",
        "        edgecolor='lime',\n",
        "        facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "# Show image info\n",
        "ax.set_title(f\"Sample image with {len(img_labels)} bounding boxes\")\n",
        "ax.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MW_fGgP4OWRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Model\n",
        "\n",
        "The original Yolo-Fastest was built using the [Darknet](https://github.com/pjreddie/darknet) framework. We need to download the Yolo-Fastest repo, convert the model to PyTorch, and load the preexisting weights."
      ],
      "metadata": {
        "id": "taYyYTldU4Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model repo (we need the model architecture and weights)\n",
        "if not REPO_PATH.exists():\n",
        "    !curl -L -o {REPO_ZIP_PATH} {REPO_URL}\n",
        "\n",
        "    # Extract and rename to consistent path\n",
        "    with zipfile.ZipFile(REPO_ZIP_PATH, 'r') as zip_ref:\n",
        "        top_folder = zip_ref.namelist()[0].split('/')[0]\n",
        "        zip_ref.extractall(REPO_PATH.parent)\n",
        "\n",
        "    # Rename extracted folder to desired name\n",
        "    extracted_path = REPO_PATH.parent / top_folder\n",
        "    extracted_path.rename(REPO_PATH)\n",
        "    print(f\"Downloaded and extracted to {REPO_PATH}\")\n",
        "else:\n",
        "    print(f\"Repo already exists at {REPO_PATH}, skipping download\")"
      ],
      "metadata": {
        "id": "m-c0OJb3S8vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the original config\n",
        "with open(MODEL_CFG_PATH, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Change input size strings in config\n",
        "new_content = content.replace(\"width=320\", f\"width={IMG_WIDTH}\")\n",
        "new_content = new_content.replace(\"height=320\", f\"height={IMG_HEIGHT}\")\n",
        "\n",
        "# Save modified config\n",
        "with open(MODIFIED_CFG_PATH, 'w') as f:\n",
        "    f.write(new_content)\n",
        "\n",
        "# Verify changes\n",
        "with open(MODIFIED_CFG_PATH, 'r') as f:\n",
        "    first_lines = ''.join(f.readlines()[:15])\n",
        "print(f\"First lines of modified config:\\n{first_lines}\")"
      ],
      "metadata": {
        "id": "ZjHKHeVW4M1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download converter script\n",
        "urllib.request.urlretrieve(CONVERTER_SCRIPT_URL, CONVERTER_SCRIPT_PATH)\n",
        "print(f\"Downloaded converter script to {CONVERTER_SCRIPT_PATH}\")\n",
        "\n",
        "# Import classes and functions from our converter script\n",
        "from yolo_fastest_to_pytorch import YoloFastest, ConvNoBN\n"
      ],
      "metadata": {
        "id": "NUWY4ITkC9Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "model = YoloFastest(str(MODIFIED_CFG_PATH), input_size=IMG_HEIGHT)\n",
        "print(f\"Model built! Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Load weights\n",
        "model.load_darknet_weights(str(MODEL_WEIGHTS_PATH))\n",
        "\n",
        "# Test forward pass\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    dummy = torch.randn(1, 3, IMG_HEIGHT, IMG_WIDTH)\n",
        "    outputs = model(dummy)\n",
        "    print(f\"Output shapes: {[o.shape for o in outputs]}\")"
      ],
      "metadata": {
        "id": "aue4I6LdBOjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replace Detection Heads\n",
        "\n",
        "We need to replace the detection heads (final layers that predict one of 80 COCO classes by default) with custom layers that predict our 1 class (\"person\").\n",
        "\n",
        "YOLO-Fastest has 2 detection heads that output predictions on two different grids (a coarse 6x6 grid and a finer 12x12 grid). Each grid cell contains 3 anchor points where an object might appear (anchors are predefined bounding boxes with different aspect ratios). The output of each detection head is a tensor with shape `[batch, channels, grid_height, grid_width]` where:\n",
        "\n",
        "* `batch`: number of images. For inference, this will be 1.\n",
        "* `channels`: 18 values representing 3 anchors with 6 predictions per anchor:\n",
        "  * `tx, ty`: box center offset (relative to grid cell)\n",
        "  * `tw, th`: box width/height scaling (relative to anchor size)\n",
        "  * `obj`: objectness score (confidence that an object exists)\n",
        "  * `cls`: class probability (\"person\")\n",
        "* `grid_height`: number of rows of cells\n",
        "* `grid_width`: number of columns of cells\n",
        "\n",
        "The original COCO detection heads output 255 channels (3 anchors × 85 values, where 85 = 4 box + 1 obj + 80 classes). We replace these with heads that output 18 channels (3 anchors × 6 values, where 6 = 4 box + 1 obj + 1 class).\n",
        "\n",
        "We should expect the output of the coarse detection head to have the shape `[1, 18, 6, 6]` and the output of the finer detection head to have the shape `[1, 18, 12, 12]`."
      ],
      "metadata": {
        "id": "TdLO8uWZG4UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_detection_heads(model):\n",
        "    \"\"\"Find indices of conv layers immediately before yolo layers.\"\"\"\n",
        "    head_indices = []\n",
        "    layer_idx = 0\n",
        "\n",
        "    for i, block in enumerate(model.blocks):\n",
        "        if block['type'] == 'net':\n",
        "            continue\n",
        "\n",
        "        if block['type'] == 'yolo':\n",
        "            # The conv layer before this yolo is a detection head\n",
        "            # Search backwards for the most recent conv\n",
        "            search_idx = layer_idx - 1\n",
        "            while search_idx >= 0:\n",
        "                # Check the block type at this index\n",
        "                block_idx = 0\n",
        "                for b in model.blocks:\n",
        "                    if b['type'] == 'net':\n",
        "                        continue\n",
        "                    if block_idx == search_idx:\n",
        "                        if b['type'] == 'convolutional':\n",
        "                            head_indices.append(search_idx)\n",
        "                        break\n",
        "                    block_idx += 1\n",
        "                break\n",
        "\n",
        "        layer_idx += 1\n",
        "\n",
        "    return head_indices"
      ],
      "metadata": {
        "id": "6HeeZkUUBPGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find detection head layers programmatically\n",
        "head_indices = find_detection_heads(model)\n",
        "print(f\"Detection head layer indices: {head_indices}\")"
      ],
      "metadata": {
        "id": "9dD2qKDSFZah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace detection heads with our 1-class detection heads\n",
        "for idx in head_indices:\n",
        "    old_conv = model.module_list[idx].conv\n",
        "    model.module_list[idx] = ConvNoBN(\n",
        "        in_ch=old_conv.in_channels,\n",
        "        out_ch=NEW_FILTERS,\n",
        "        kernel=old_conv.kernel_size[0],\n",
        "        stride=old_conv.stride[0],\n",
        "        pad=old_conv.padding[0],\n",
        "        activation='linear'\n",
        "    )\n",
        "\n",
        "# Verify new output shapes\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    dummy = torch.randn(1, 3, IMG_HEIGHT, IMG_WIDTH)\n",
        "    outputs = model(dummy)\n",
        "    print(f\"New output shapes: {[o.shape for o in outputs]}\")"
      ],
      "metadata": {
        "id": "TLr7TsMnF0La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to device\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "RgZstHGAGGTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replace Leaky ReLU with ReLU6\n",
        "\n",
        "The Yolo-Fastest model uses [Leaky ReLU](https://docs.pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html) activation functions, which can be tricky for the RUHMI framework to convert to NPU code. To avoid any compilation issues later, we replace all the *Leaky ReLU* functions with [ReLU6](https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU6.html) functions (which essentially clamps inputs to `[0, 6]`). You might lose a little accuracy, but it should help ensure the model runs on the NPU."
      ],
      "metadata": {
        "id": "S_2HPqln4ePi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_leaky_relu_with_relu6(module):\n",
        "    \"\"\"Recursively replace LeakyReLU with ReLU6 in a module.\"\"\"\n",
        "    count = 0\n",
        "    for name, child in module.named_children():\n",
        "        if isinstance(child, nn.LeakyReLU):\n",
        "            setattr(module, name, nn.ReLU6(inplace=True))\n",
        "            count += 1\n",
        "        else:\n",
        "            count += replace_leaky_relu_with_relu6(child)\n",
        "    return count"
      ],
      "metadata": {
        "id": "B3hX-F7H4gG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count Leaky ReLU layers before replacement\n",
        "leaky_relu_count = sum(1 for m in model.modules() if isinstance(m, nn.LeakyReLU))\n",
        "print(f\"Found {leaky_relu_count} LeakyReLU layers in the model\")\n",
        "\n",
        "# Replace them\n",
        "replaced_count = replace_leaky_relu_with_relu6(model)\n",
        "print(f\"Replaced {replaced_count} LeakyReLU layers with ReLU6\")\n",
        "\n",
        "# Verify no LeakyReLU remains\n",
        "remaining = sum(1 for m in model.modules() if isinstance(m, nn.LeakyReLU))\n",
        "relu6_count = sum(1 for m in model.modules() if isinstance(m, nn.ReLU6))\n",
        "print(f\"Verification: {remaining} LeakyReLU remaining, {relu6_count} ReLU6 layers now present\")"
      ],
      "metadata": {
        "id": "6De87pGs4hyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freeze Parts of the Model\n",
        "\n",
        "You have the option to *freeze* sections of the model. Freezing preserves the pretrained feature extraction capabilities, preventing the model from \"forgetting\" what it learned on COCO. This is useful when your dataset is small (risk of overfitting) or very similar to the original training data (features already work well). Freezing also speeds up training since fewer gradients need to be computed. You have the option of freezing either:\n",
        "\n",
        "* **Backbone:** Extracts basic visual features from the image (e.g., edges, textures, shapes, object parts). YOLO-Fastest uses depthwise separable convolutions and residual blocks for efficient feature extraction. These low-level features are generally universal across datasets.\n",
        "* **Neck:** Fuses features from different scales to help detect both large and small objects. This includes:\n",
        "  * *Spatial Pyramid Pooling (SPP)*: Captures multi-scale context using parallel pooling operations at different kernel sizes\n",
        "  * *FPN-like layers*: Upsamples and concatenates features from earlier backbone layers, combining semantic (what) and spatial (where) information\n",
        "\n",
        "Note that the **detection heads** are single convolutional layers that output predictions for each grid cell. Each prediction contains box coordinates, objectness score, and class probability — all combined into 18 channels (3 anchors × 6 values). We replaced these heads for our single class, and they will always remain trainable.\n",
        "\n",
        "A good starting point is to freeze both backbone and neck, training only the detection heads. If performance is insufficient, try unfreezing the neck to allow the feature fusion layers to adapt to your data."
      ],
      "metadata": {
        "id": "U3j9lpbUT_PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_model_layers(model, freeze_backbone=True, freeze_neck=True):\n",
        "    \"\"\"\n",
        "    Freeze portions of the YOLO-Fastest model. Note that the layers are hardcoded.\n",
        "\n",
        "    Architecture breakdown:\n",
        "    - Backbone: layers 0-108 (feature extraction)\n",
        "    - Neck/SPP: layers 109-119, 122-128 (feature fusion)\n",
        "    - Detection heads: layers 120, 129 (class-specific, always trainable)\n",
        "    \"\"\"\n",
        "    # Find detection head indices (layers with output matching our NEW_FILTERS)\n",
        "    head_indices = set()\n",
        "    for i, module in enumerate(model.module_list):\n",
        "        if hasattr(module, 'conv') and module.conv.out_channels == NEW_FILTERS:\n",
        "            head_indices.add(i)\n",
        "\n",
        "    # Define layer ranges\n",
        "    backbone_range = range(0, 109)  # Layers 0-108\n",
        "    neck_range = list(range(109, 120)) + list(range(122, 129))  # Exclude heads\n",
        "\n",
        "    # Freeze backbone\n",
        "    if freeze_backbone:\n",
        "        for i in backbone_range:\n",
        "            for param in model.module_list[i].parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Freeze neck\n",
        "    if freeze_neck:\n",
        "        for i in neck_range:\n",
        "            for param in model.module_list[i].parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Ensure detection heads are always trainable\n",
        "    for i in head_indices:\n",
        "        for param in model.module_list[i].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    return head_indices"
      ],
      "metadata": {
        "id": "77AgxLe3TzO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply freezing\n",
        "head_indices = freeze_model_layers(model, FREEZE_BACKBONE, FREEZE_NECK)\n",
        "print(f\"Backbone frozen: {FREEZE_BACKBONE}\")\n",
        "print(f\"Neck frozen: {FREEZE_NECK}\")\n",
        "\n",
        "# Count trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Parameters: {trainable_params:,} trainable / {total_params:,} total\")"
      ],
      "metadata": {
        "id": "zvv5TA61UzVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Loss Function\n",
        "\n",
        "We will use some helper functions from the custom [yolo_fastest_utils.py](https://github.com/ShawnHymel/intro-to-edge-ai-and-cv-with-renesas/blob/main/scripts/yolo_fastest_utils.py) script along with the custom loss function defined in [yolo_fastest_loss.py](https://github.com/ShawnHymel/intro-to-edge-ai-and-cv-with-renesas/blob/main/scripts/yolo_fastest_loss.py).\n",
        "\n",
        "For each ground truth box, we find which grid cell contains its center and which anchor best matches its shape (based on IoU of width/height). We then compute three losses: complete IoU (CIoU) loss for box coordinates (penalizes poor overlap, center distance, and aspect ratio mismatch), binary cross-entropy (BCE) loss for objectness (does this anchor contain an object?), and BCE loss for classification (is it a person?). The losses are weighted and summed across both output scales (6×6 and 12×12 grids)."
      ],
      "metadata": {
        "id": "v-1Wn6I-WGgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download utilities script\n",
        "urllib.request.urlretrieve(UTILS_SCRIPT_URL, UTILS_SCRIPT_PATH)\n",
        "print(f\"Downloaded utils script to {UTILS_SCRIPT_PATH}\")\n",
        "\n",
        "# Import utilities\n",
        "from yolo_fastest_utils import (\n",
        "    get_anchors_from_config,\n",
        "    box_iou,\n",
        "    bbox_iou_tensor,\n",
        "    box_iou_wh,\n",
        "    build_targets,\n",
        "    decode_predictions,\n",
        "    nms,\n",
        "    compute_map\n",
        ")"
      ],
      "metadata": {
        "id": "eSZ0DKD_WGQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download loss calculation class\n",
        "urllib.request.urlretrieve(LOSS_SCRIPT_URL, LOSS_SCRIPT_PATH)\n",
        "print(f\"Downloaded loss script to {LOSS_SCRIPT_PATH}\")\n",
        "\n",
        "# Import loss function\n",
        "from yolo_fastest_loss import YoloLoss"
      ],
      "metadata": {
        "id": "IXFy33cVirNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get anchors from model config\n",
        "anchors, anchor_masks = get_anchors_from_config(model)\n",
        "print(f\"Anchors: {anchors}\")\n",
        "print(f\"Masks: {anchor_masks}\")\n",
        "\n",
        "# Create loss function\n",
        "criterion = YoloLoss(\n",
        "    anchors=anchors,\n",
        "    anchor_masks=anchor_masks,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    img_size=IMG_HEIGHT\n",
        ")\n",
        "print(f\"Grid sizes: {criterion.grid_sizes}\")"
      ],
      "metadata": {
        "id": "5fhCbDVkOcct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of images and labels from the training dataset\n",
        "model.train()\n",
        "imgs, labels = next(iter(train_loader))\n",
        "imgs = imgs.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "# Calculate losses using our custom loss function\n",
        "outputs = model(imgs)\n",
        "loss, box_loss, obj_loss, cls_loss = criterion(outputs, labels)\n",
        "\n",
        "# Print losses\n",
        "print(f\"Box:   {box_loss.item():.4f}\")\n",
        "print(f\"Obj:   {obj_loss.item():.4f}\")\n",
        "print(f\"Cls:   {cls_loss.item():.4f}\")\n",
        "print(f\"Total: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "_bZETOszhfwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "We are now ready to train our model!"
      ],
      "metadata": {
        "id": "Z60w74vjkoDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "2-HZ4PY1i9oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(\n",
        "    model,\n",
        "    train_loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    device,\n",
        "    verbose=False\n",
        "):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    total_loss = 0\n",
        "    total_box_loss = 0\n",
        "    total_obj_loss = 0\n",
        "    total_cls_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    # Enable training-specific behaviors (e.g. dropout)\n",
        "    model.train()\n",
        "\n",
        "    # Do one full training cycle on a batch of training data\n",
        "    for i, (imgs, labels) in enumerate(train_loader):\n",
        "        # Move data to the same device as the model\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(imgs)\n",
        "        loss, box_loss, obj_loss, cls_loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        total_loss += loss.item()\n",
        "        total_box_loss += box_loss.item()\n",
        "        total_obj_loss += obj_loss.item()\n",
        "        total_cls_loss += cls_loss.item()\n",
        "\n",
        "        # Optionally print progress every 50 batches\n",
        "        if verbose and (i+1) % 50 == 0:\n",
        "            print(f\"    Batch {i+1}/{num_batches} - loss: {loss.item():.2f}\")\n",
        "\n",
        "    # Calculate averages\n",
        "    return {\n",
        "        'loss': total_loss / num_batches,\n",
        "        'box': total_box_loss / num_batches,\n",
        "        'obj': total_obj_loss / num_batches,\n",
        "        'cls': total_cls_loss / num_batches\n",
        "    }"
      ],
      "metadata": {
        "id": "5ifUXf68j1Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    total_loss = 0\n",
        "    total_box_loss = 0\n",
        "    total_obj_loss = 0\n",
        "    total_cls_loss = 0\n",
        "    num_batches = len(val_loader)\n",
        "\n",
        "    # Disable training-specific behaviors (e.g. dropout)\n",
        "    model.eval()\n",
        "\n",
        "    # Do not track gradients during validation\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            # Move data to the same device as the model\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(imgs)\n",
        "            loss, box_loss, obj_loss, cls_loss = criterion(outputs, labels)\n",
        "\n",
        "            # Get total loss for the batch\n",
        "            total_loss += loss.item()\n",
        "            total_box_loss += box_loss.item()\n",
        "            total_obj_loss += obj_loss.item()\n",
        "            total_cls_loss += cls_loss.item()\n",
        "\n",
        "    # Calculate averages\n",
        "    return {\n",
        "        'loss': total_loss / num_batches,\n",
        "        'box': total_box_loss / num_batches,\n",
        "        'obj': total_obj_loss / num_batches,\n",
        "        'cls': total_cls_loss / num_batches\n",
        "    }"
      ],
      "metadata": {
        "id": "3Q8IGNoflPuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "history = {\n",
        "    'train_loss': [], 'train_box': [], 'train_obj': [], 'train_cls': [],\n",
        "    'val_loss': [], 'val_box': [], 'val_obj': [], 'val_cls': []\n",
        "}\n",
        "\n",
        "# Train for a number of epochs, save best model (lowest val loss)\n",
        "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    # Train\n",
        "    train_metrics = train_one_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        device,\n",
        "        True\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    val_metrics = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    # Record history\n",
        "    history['train_loss'].append(train_metrics['loss'])\n",
        "    history['train_box'].append(train_metrics['box'])\n",
        "    history['train_obj'].append(train_metrics['obj'])\n",
        "    history['train_cls'].append(train_metrics['cls'])\n",
        "    history['val_loss'].append(val_metrics['loss'])\n",
        "    history['val_box'].append(val_metrics['box'])\n",
        "    history['val_obj'].append(val_metrics['obj'])\n",
        "    history['val_cls'].append(val_metrics['cls'])\n",
        "\n",
        "    # Save best model\n",
        "    if val_metrics['loss'] < best_val_loss:\n",
        "        best_val_loss = val_metrics['loss']\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - \"\n",
        "              f\"Train: {train_metrics['loss']:.4f} - \"\n",
        "              f\"Val: {val_metrics['loss']:.4f} - \"\n",
        "              f\"Saved best model\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - \"\n",
        "              f\"Train: {train_metrics['loss']:.4f} - \"\n",
        "              f\"Val: {val_metrics['loss']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training complete!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "AMjZTi37lRXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Total loss\n",
        "axes[0].plot(history['train_loss'], label='Train')\n",
        "axes[0].plot(history['val_loss'], label='Validation')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Total Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Component losses\n",
        "axes[1].plot(history['train_box'], label='Train Box')\n",
        "axes[1].plot(history['val_box'], label='Val Box')\n",
        "axes[1].plot(history['train_obj'], label='Train Obj')\n",
        "axes[1].plot(history['val_obj'], label='Val Obj')\n",
        "axes[1].plot(history['train_cls'], label='Train Cls')\n",
        "axes[1].plot(history['val_cls'], label='Val Cls')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Loss Components')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YL1wemP6nrdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Model\n",
        "\n",
        "We will compute the mean average precision at 50% IoU (mAP@0.5) using our test set. To do that, we need write a few helper functions for computing the non-maximum suppression (NMS) and intersection over union (IoU).\n",
        "\n",
        "We'll also see how the model performs on a single image by drawing the ground-truth bounding boxes and comparing them to the predicted bounding boxes."
      ],
      "metadata": {
        "id": "Q1E81LhFGYNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model weights and switch to evaluation mode\n",
        "model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
        "model.eval()\n",
        "print(f\"Loaded model from {BEST_MODEL_PATH}\")"
      ],
      "metadata": {
        "id": "KD5jm_H1GDs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mAP on test set\n",
        "test_map, total_preds, total_gt = compute_map(\n",
        "    model=model,\n",
        "    data_loader=test_loader,\n",
        "    anchors=anchors,\n",
        "    anchor_masks=anchor_masks,\n",
        "    img_size=IMG_HEIGHT,\n",
        "    device=device,\n",
        "    conf_thresh=0.25,\n",
        "    iou_thresh=0.5\n",
        ")\n",
        "\n",
        "print(f\"Test mAP@0.5: {test_map:.4f}\")\n",
        "print(f\"Total predictions: {total_preds}\")\n",
        "print(f\"Total ground truth: {total_gt}\")"
      ],
      "metadata": {
        "id": "MRaeJb9FGa3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Model\n",
        "\n",
        "Note that in most cases, the export process will produce 2 separate files:\n",
        "* **.onnx** - Model architecture and metadata (with references to external weight data)\n",
        "* **.onnx.data** - Model weights (external data file)"
      ],
      "metadata": {
        "id": "CSdmnJJwHvGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save full PyTorch model\n",
        "model.eval()\n",
        "torch.save(model, PYTORCH_PATH)\n",
        "print(f\"Saved PyTorch model to: {PYTORCH_PATH}\")"
      ],
      "metadata": {
        "id": "z5PvdSpMOPtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the model into evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create a dummy input tensor with the same shape as one sample (batch=1)\n",
        "dummy_input = torch.randn(1, 3, IMG_HEIGHT, IMG_WIDTH).to(device)\n",
        "\n",
        "# Export to ONNX\n",
        "torch.onnx.export(\n",
        "    model,                              # Model to export\n",
        "    dummy_input,                        # Example input (for tracing)\n",
        "    ONNX_PATH,                          # Output file path\n",
        "    export_params=True,                 # Export with trained weights\n",
        "    opset_version=ONNX_OPSET_VERSION,   # Which operations are supported\n",
        "    do_constant_folding=True,           # Optimize constant operations\n",
        "    input_names=['input'],              # Name for input layer\n",
        "    output_names=['output'],            # Name for output layer\n",
        "    dynamic_axes=None,                  # Fixed batch size of 1\n",
        "    dynamo=False,                       # Force legacy exporter\n",
        ")\n",
        "print(f\"Model exported to: {ONNX_PATH}\")"
      ],
      "metadata": {
        "id": "IfndCDVTHIUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Calibration Data\n",
        "\n",
        "Export a few samples from the validation set to act as calibration data for post-training quantization."
      ],
      "metadata": {
        "id": "AM2Y7Di50Fk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't exceed the total number of available samples\n",
        "num_samples = min(NUM_CALIB_SAMPLES, len(val_dataset))\n",
        "\n",
        "# Randomly choose from validation set\n",
        "indices = random.sample(range(len(val_dataset)), num_samples)\n",
        "\n",
        "# Get samples (ignore the labels) and convert to NumPy arrays (float32 format)\n",
        "calib_samples = []\n",
        "for i in indices:\n",
        "    x, _ = val_dataset[i]\n",
        "    calib_samples.append(x.float().numpy())\n",
        "\n",
        "# Stack into a single array: shape (num_samples, 3, H, W)\n",
        "calib_data = np.stack(calib_samples, axis=0)\n",
        "\n",
        "# Save samples as NPZ\n",
        "np.savez(CALIB_NPZ_PATH, input=calib_data)\n",
        "print(f\"Calibration data shape: {calib_data.shape}\")\n",
        "print(f\"Calibration data dtype: {calib_data.dtype}\")\n",
        "print(f\"Calibration data range: [{calib_data.min():.1f}, {calib_data.max():.1f}]\")\n",
        "print(f\"Saved calibration data to: {CALIB_NPZ_PATH}\")"
      ],
      "metadata": {
        "id": "GM4YaoiVzLqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to TFLite\n",
        "\n",
        "Unfortunately, the quantization process in RUHMI can cause some catastrophic losses that propagate through the model, resulting in large errors in the output. While it works for smaller models, deeper networks (like Yolo-Fastest) can end up with large output errors due to these quantization issues.\n",
        "\n",
        "The workaround is to reconstruct the model in TensorFlow and quantize and convert to a LiteRT file (.tflite) before using RUHMI/MERA to export it as NPU code. Because the model is already quantized, you can simply have RUHMI perform the conversion step, skipping quantization."
      ],
      "metadata": {
        "id": "7UnFZ8VgQ0zV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download loss calculation class\n",
        "urllib.request.urlretrieve(TFLITE_SCRIPT_URL, TFLITE_SCRIPT_PATH)\n",
        "print(f\"Downloaded loss script to {TFLITE_SCRIPT_PATH}\")\n",
        "\n",
        "# Import conversion functionns\n",
        "from yolo_fastest_to_tflite import (\n",
        "    print_raw_outputs,\n",
        "    run_tflite_inference,\n",
        "    yolo_fastest_to_quantized_tflite,\n",
        ")"
      ],
      "metadata": {
        "id": "cVQkefDIPSO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert model to quantized tflite file\n",
        "info = yolo_fastest_to_quantized_tflite(\n",
        "    pt_model_path=PYTORCH_PATH,\n",
        "    calibration_data_path=CALIB_NPZ_PATH,\n",
        "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
        "    output_path=TFLITE_PATH,\n",
        ")"
      ],
      "metadata": {
        "id": "74lEERKYPSMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Helper Code\n",
        "\n",
        "We need to remember our image resolution, quantization details, classes, and anchor points. Copy this generated C code to your firmware application."
      ],
      "metadata": {
        "id": "fqWbZiWlPcBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract grid sizes from info keys (sorted: coarse first, fine second)\n",
        "grid_sizes = sorted(set(\n",
        "    int(k.split(\"_\")[1].split(\"x\")[0]) for k in info if k.startswith(\"output_\") and \"x\" in k.split(\"_\")[1]\n",
        "))\n",
        "grid_1 = grid_sizes[0]  # Coarse grid (smaller spatial dim)\n",
        "grid_2 = grid_sizes[1]  # Fine grid (larger spatial dim)\n",
        "\n",
        "# Print grid sizes\n",
        "print(f\"Grid 1 size: {grid_1}x{grid_1}\")\n",
        "print(f\"Grid 2 size: {grid_2}x{grid_2}\")"
      ],
      "metadata": {
        "id": "oMIewO2AHe8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate C code with settings for firmware\n",
        "c_code = f\"\"\"\\\n",
        "/* Target image dimensions */\n",
        "#define IMG_WIDTH {IMG_WIDTH}\n",
        "#define IMG_HEIGHT {IMG_HEIGHT}\n",
        "#define IMG_CHANNELS 3\n",
        "\n",
        "/* YOLO detection settings */\n",
        "#define NUM_CLASSES {NUM_CLASSES}\n",
        "#define NUM_ANCHORS {NUM_ANCHORS}\n",
        "#define NUM_ATTRS 6     /* tx, ty, tw, th, obj, cls */\n",
        "#define CONF_THRESHOLD 0.25f\n",
        "#define IOU_THRESHOLD 0.45f\n",
        "#define MAX_DETECTIONS 10\n",
        "\n",
        "/* Grid sizes */\n",
        "#define GRID_1_SIZE {grid_1}   /* Coarse grid for larger objects */\n",
        "#define GRID_2_SIZE {grid_2}  /* Fine grid for smaller objects */\n",
        "\n",
        "/* Quantization parameters */\n",
        "#define INPUT_SCALE {info['input_scale']:.10f}f\n",
        "#define INPUT_ZP {info['input_zp']}\n",
        "#define OUTPUT_GRID1_SCALE {info[f'output_{grid_1}x{grid_1}_scale']:.10f}f\n",
        "#define OUTPUT_GRID1_ZP {info[f'output_{grid_1}x{grid_1}_zp']}\n",
        "#define OUTPUT_GRID2_SCALE {info[f'output_{grid_2}x{grid_2}_scale']:.10f}f\n",
        "#define OUTPUT_GRID2_ZP {info[f'output_{grid_2}x{grid_2}_zp']}\n",
        "\n",
        "/* Anchor masks */\n",
        "/* Grid 1 ({grid_1}x{grid_1}) uses anchors: {anchor_masks[0]} */\n",
        "/* Grid 2 ({grid_2}x{grid_2}) uses anchors: {anchor_masks[1]} */\n",
        "\n",
        "/* Anchors for grid 1 ({grid_1}x{grid_1} - larger objects) */\n",
        "static const float anchors_grid1[NUM_ANCHORS][2] = {{\n",
        "\"\"\"\n",
        "\n",
        "# Add grid 1 anchors\n",
        "for idx in anchor_masks[0]:\n",
        "    w, h = anchors[idx]\n",
        "    c_code += f\"    {{{w:.1f}f, {h:.1f}f}},  /* anchor {idx} */\\n\"\n",
        "c_code += \"};\\n\\n\"\n",
        "\n",
        "# Add grid 2 anchors\n",
        "c_code += f\"/* Anchors for grid 2 ({grid_2}x{grid_2} - smaller objects) */\\n\"\n",
        "c_code += \"static const float anchors_grid2[NUM_ANCHORS][2] = {\\n\"\n",
        "for idx in anchor_masks[1]:\n",
        "    w, h = anchors[idx]\n",
        "    c_code += f\"    {{{w:.1f}f, {h:.1f}f}},  /* anchor {idx} */\\n\"\n",
        "c_code += \"};\\n\\n\"\n",
        "\n",
        "# Add class names\n",
        "c_code += \"/* Labels */\\n\"\n",
        "c_code += \"const char* class_names[] = {\\n\"\n",
        "c_code += \"    \\\"person\\\",\\n\"\n",
        "c_code += \"};\"\n",
        "\n",
        "print(c_code)"
      ],
      "metadata": {
        "id": "K93cAnltPdQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Inference Static Sample\n",
        "\n",
        "Compare the output of inference from the original model and the quantized/deployed model on the embedded device."
      ],
      "metadata": {
        "id": "E9TUCd7iPkNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose an index into the test dataset\n",
        "idx = 3\n",
        "\n",
        "# Get a single test image directly from dataset\n",
        "img, label = test_dataset[idx]\n",
        "\n",
        "# Prepare image for display (already RGB, values in [0, 1])\n",
        "img_np = img.permute(1, 2, 0).numpy()\n",
        "\n",
        "# Run inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    img_input = img.unsqueeze(0).float().to(device)  # Add batch dimension\n",
        "    outputs = model(img_input)\n",
        "\n",
        "# Decode predictions and apply NMS\n",
        "predictions = decode_predictions(\n",
        "    outputs,\n",
        "    anchors=anchors,\n",
        "    anchor_masks=anchor_masks,\n",
        "    img_size=IMG_HEIGHT,\n",
        "    conf_thresh=0.25\n",
        ")\n",
        "pred_boxes = nms(predictions[0], iou_thresh=0.45)\n",
        "\n",
        "# Print raw outputs\n",
        "print_raw_outputs(\n",
        "    output_0=outputs[0][0].cpu().numpy(),\n",
        "    output_1=outputs[1][0].cpu().numpy(),\n",
        "    format=\"chw\",\n",
        "    input_image=img.numpy(),\n",
        "    predictions=pred_boxes,\n",
        "    ground_truth=label,\n",
        ")"
      ],
      "metadata": {
        "id": "kG8HFfAHGfwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
        "ax.imshow(img_np)  # matplotlib handles [0, 1] float range\n",
        "\n",
        "# Draw ground truth boxes (green)\n",
        "# Note: label shape is [N, 5] with (class, cx, cy, w, h) - no batch_idx\n",
        "for lbl in label:\n",
        "    cx = lbl[1].item() * IMG_WIDTH\n",
        "    cy = lbl[2].item() * IMG_HEIGHT\n",
        "    w = lbl[3].item() * IMG_WIDTH\n",
        "    h = lbl[4].item() * IMG_HEIGHT\n",
        "    x = cx - w / 2\n",
        "    y = cy - h / 2\n",
        "    rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='lime', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "# Draw predicted boxes (red)\n",
        "for pred in pred_boxes:\n",
        "    x1, y1, x2, y2, conf, cls = pred\n",
        "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(x1, y1-5, f'{conf:.2f}', color='red', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.set_title(f\"Green: Ground Truth, Red: Predictions ({len(pred_boxes)} detections)\")\n",
        "ax.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Ground truth boxes: {len(label)}\")\n",
        "print(f\"Predicted boxes: {len(pred_boxes)}\")"
      ],
      "metadata": {
        "id": "dq4wMrgfWMxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save test image as NumPy array\n",
        "np.savez(\n",
        "    TEST_SAMPLE_NPZ_PATH,\n",
        "    image=img.numpy(),\n",
        "    label=label.numpy(),        # ground truth boxes\n",
        "    pt_out0=outputs[0].cpu().numpy(),\n",
        "    pt_out1=outputs[1].cpu().numpy(),\n",
        ")\n",
        "print(\"Test image and PyTorch reference outputs saved\")"
      ],
      "metadata": {
        "id": "lJuX8jlqa2u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get image as numpy array: (3, H, W) in [0, 1] range (CHW format from PyTorch)\n",
        "sample_img = img.numpy()\n",
        "\n",
        "# Convert CHW to HWC format to match firmware's image_rgb565_to_rgb888() output\n",
        "sample_img_hwc = np.transpose(sample_img, (1, 2, 0))\n",
        "\n",
        "# Convert to uint8 [0, 255] range\n",
        "sample_img_uint8 = (sample_img_hwc * 255).astype(np.uint8)\n",
        "\n",
        "# Generate header comment with expected outputs\n",
        "c_code = f\"\"\"/**\n",
        " * Static test input for person detection model\n",
        " *\n",
        " * Image: RGB, {IMG_WIDTH}x{IMG_HEIGHT}\n",
        " * Format: HWC (height, width, channels) - matches firmware camera output\n",
        " * Values: 0-255 (uint8)\n",
        " *\n",
        " * Expected detections (from PyTorch model):\n",
        "\"\"\"\n",
        "\n",
        "for i, pred in enumerate(pred_boxes):\n",
        "    x1, y1, x2, y2, conf, cls = pred\n",
        "    c_code += f\" *   Box {i}: ({x1:.1f}, {y1:.1f}) to ({x2:.1f}, {y2:.1f}), conf={conf:.4f}\\n\"\n",
        "if len(pred_boxes) == 0:\n",
        "    c_code += \" *   No detections\\n\"\n",
        "\n",
        "c_code += f\"\"\" */\n",
        "\n",
        "#include <stdint.h>\n",
        "\n",
        "/* Image dimensions */\n",
        "#define TEST_IMG_WIDTH {IMG_WIDTH}\n",
        "#define TEST_IMG_HEIGHT {IMG_HEIGHT}\n",
        "#define TEST_IMG_CHANNELS 3\n",
        "#define TEST_INPUT_SIZE (TEST_IMG_WIDTH * TEST_IMG_HEIGHT * TEST_IMG_CHANNELS)\n",
        "\n",
        "/* Input data (RGB, HWC format, uint8) */\n",
        "const uint8_t test_input[TEST_INPUT_SIZE] = {{\"\"\"\n",
        "\n",
        "# Add pixel values - flatten HWC array (row by row, RGB interleaved)\n",
        "flat_data = sample_img_uint8.flatten()\n",
        "for i, val in enumerate(flat_data):\n",
        "    if i % 16 == 0:\n",
        "        c_code += \"\\n    \"\n",
        "    c_code += f\"{val:3d}, \"\n",
        "c_code += \"\\n};\"\n",
        "\n",
        "# Save to file\n",
        "with open(TEST_SAMPLE_H_PATH, 'w') as f:\n",
        "    f.write(c_code)\n",
        "\n",
        "print(f\"Saved test sample to: {TEST_SAMPLE_H_PATH}\")\n",
        "print(f\"File size: {len(c_code):,} characters\")\n",
        "print(f\"Image data: {len(flat_data):,} bytes\")"
      ],
      "metadata": {
        "id": "JZ4uV2fxPoBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Quantization\n",
        "\n",
        "Run the test sample through the quantized TFLite model file and check the raw outputs against the PyTorch raw outputs. Expect some slight variance, as the quantization process introduces some errors/noise in the model. You can also compare these outputs to the ones generated by the microcontroller code."
      ],
      "metadata": {
        "id": "Mc40cZuyXq96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare TFLite INT8 outputs with PyTorch FP32\n",
        "results = run_tflite_inference(\n",
        "    tflite_path=TFLITE_PATH,\n",
        "    test_npz_path=TEST_SAMPLE_NPZ_PATH,\n",
        ")\n",
        "\n",
        "# Get grid names sorted by size (coarse first, fine second)\n",
        "grids = sorted(results[\"outputs\"].keys(), key=lambda g: int(g.split(\"x\")[0]))\n",
        "\n",
        "# Convert HWC numpy -> NCHW tensors for decode_predictions\n",
        "tflite_outputs = [\n",
        "    torch.tensor(results[\"outputs\"][g]).permute(2, 0, 1).unsqueeze(0)\n",
        "    for g in grids\n",
        "]\n",
        "\n",
        "# Decode predictions and apply NMS\n",
        "tflite_predictions = decode_predictions(\n",
        "    tflite_outputs,\n",
        "    anchors=anchors,\n",
        "    anchor_masks=anchor_masks,\n",
        "    img_size=IMG_HEIGHT,\n",
        "    conf_thresh=0.25\n",
        ")\n",
        "tflite_pred_boxes = nms(tflite_predictions[0], iou_thresh=0.45)\n",
        "\n",
        "# Print raw outputs with decoded boxes\n",
        "print_raw_outputs(\n",
        "    output_0=results[\"outputs\"][grids[0]],\n",
        "    output_1=results[\"outputs\"][grids[1]],\n",
        "    format=\"hwc\",\n",
        "    predictions=tflite_pred_boxes,\n",
        ")\n",
        "\n",
        "# Per-grid quantization error vs PyTorch\n",
        "print(\"\\nQuantization error vs PyTorch FP32:\")\n",
        "for grid, stats in results[\"comparison\"].items():\n",
        "    print(f\"  {grid}: MSE={stats['mse']:.4f}, max_diff={stats['max_diff']:.4f}\")"
      ],
      "metadata": {
        "id": "N3tZyUlCXquR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy!\n",
        "\n",
        "Download your *model.onnx* and *model.onnx.data* files along with the *calibration_data.npz* file. Use your vendor's toolset (e.g. RUHMI) to quantize, compress, and compile the model for your target device."
      ],
      "metadata": {
        "id": "B4K_Cv5A0KNK"
      }
    }
  ]
}