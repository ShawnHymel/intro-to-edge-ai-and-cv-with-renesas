{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Thumbs Up/Down Image Classification\n",
        "\n",
        "Welcome to the thumbs up/down image classification training script! This was designed to run in Google Colab, but it should (in theory) run in any Jupyter Notebook or Jupyter Lab environment with PyTorch installed.\n",
        "\n",
        "In Google Colab, select **File > Open notebook** then select the **Upload** tab. Select this file to open it in Colab.\n",
        "\n",
        "Press **shift + enter** to execute each cell in order. Make sure you stop and read each text section, as there are some manual steps you will need to perform (e.g. upload dataset)."
      ],
      "metadata": {
        "id": "5bIkGT6rC3i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install specific versions of the packages\n",
        "!python3 -m pip install \\\n",
        "    matplotlib=='3.10.0' \\\n",
        "    numpy=='2.0.2' \\\n",
        "    onnxscript=='0.5.7' \\\n",
        "    pandas=='2.2.2' \\\n",
        "    Pillow=='11.3.0' \\\n",
        "    torch=='2.9.0+cpu'"
      ],
      "metadata": {
        "id": "K3VVPig7C2Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import standard libraries\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "# Import third-party libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "qCiL4qs6C-k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the versions of the libraries\n",
        "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Pillow version: {Image.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ],
      "metadata": {
        "id": "4SVEpoEGDDTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General settings\n",
        "SEED = 42\n",
        "DATASET_ZIP_PATH = Path(\"/content/thumbs_up_down_images.zip\")\n",
        "DATASET_PATH = Path(\"/content/thumbs_up_down_images\")\n",
        "\n",
        "# Image preprocessing settings\n",
        "IMG_WIDTH = 48\n",
        "IMG_HEIGHT = 48\n",
        "\n",
        "# Grayscale conversion coefficients\n",
        "GRAY_R_COEFF = 0.299\n",
        "GRAY_G_COEFF = 0.587\n",
        "GRAY_B_COEFF = 0.114\n",
        "\n",
        "# Data settings\n",
        "VAL_SPLIT = 0.2\n",
        "TEST_SPLIT = 0.2\n",
        "\n",
        "# Model settings\n",
        "CONV_1_NUM_FILTERS = 8\n",
        "CONV_2_NUM_FILTERS = 16\n",
        "DROPOUT = 0.3\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "# ONNX export settings\n",
        "ONNX_OPSET_VERSION = 18\n",
        "ONNX_PATH = Path(\"/content/model.onnx\")\n",
        "\n",
        "# Calibration data settings\n",
        "NUM_CALIB_SAMPLES = 100\n",
        "CALIB_NPZ_PATH = Path(\"/content/calibration_data.npz\")"
      ],
      "metadata": {
        "id": "DXImjXTKDFKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Set random seeds for reproducibility\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "U2RoEKTcDtKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target compute device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "O1aJqO0fJmY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset\n",
        "\n",
        "You will need to manually upload the image dataset you created. This should include at least 100 samples (BMP format) for each of the 4 classes (_background, _other, thumbs_down, thumbs_up).\n",
        "\n",
        "First, zip your dataset as follows:\n",
        "\n",
        "```\n",
        "thumbs_up_down_dataset.zip\n",
        "├─ _background/\n",
        "│  ├─ 0000.BMP\n",
        "│  ├─ 0001.BMP\n",
        "│  └─ ...\n",
        "├─ _other/\n",
        "│  ├─ 0000.BMP\n",
        "│  ├─ 0001.BMP\n",
        "│  └─ ...\n",
        "├─ thumbs_down/\n",
        "│  ├─ 0000.BMP\n",
        "│  ├─ 0001.BMP\n",
        "│  └─ ...\n",
        "└─ thumbs_up/\n",
        "│  ├─ 0000.BMP\n",
        "│  ├─ 0001.BMP\n",
        "   └─ ...\n",
        "```\n",
        "\n",
        "On the left side, click the **Folder** icon to expand the file browser tab. Click the **Upload** icon. Select the **thumbs_up_down_dataset.zip** file to upload it to this Colab instance.\n",
        "\n",
        "Once you have uploaded the .zip file, run the following cells to unzip and prepare the dataset."
      ],
      "metadata": {
        "id": "ZrHPNhCqJrkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the zip file\n",
        "with zipfile.ZipFile(DATASET_ZIP_PATH, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATASET_PATH)"
      ],
      "metadata": {
        "id": "-DGOn_PAJny8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discover class names based on folder names\n",
        "class_names = sorted([d for d in os.listdir(DATASET_PATH)\n",
        "                      if os.path.isdir(os.path.join(DATASET_PATH, d))])\n",
        "print(f\"Class names: {class_names}\")"
      ],
      "metadata": {
        "id": "xHxf9rFOKgLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map an index number to each class name\n",
        "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
        "print(class_to_idx)"
      ],
      "metadata": {
        "id": "89b9pC38LVf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_nearest_neighbor(image, new_width, new_height):\n",
        "    \"\"\"\n",
        "    Resize image using nearest-neighbor interpolation\n",
        "    \"\"\"\n",
        "    old_height, old_width = image.shape[:2]\n",
        "\n",
        "    # Handle grayscale vs RGB\n",
        "    if len(image.shape) == 3:\n",
        "        resized = np.zeros((new_height, new_width, image.shape[2]), dtype=image.dtype)\n",
        "    else:\n",
        "        resized = np.zeros((new_height, new_width), dtype=image.dtype)\n",
        "\n",
        "    # Calculate scale factors\n",
        "    x_scale = old_width / new_width\n",
        "    y_scale = old_height / new_height\n",
        "\n",
        "    # Map each new pixel to nearest old pixel\n",
        "    for y in range(new_height):\n",
        "        old_y = int(y * y_scale)\n",
        "        for x in range(new_width):\n",
        "            old_x = int(x * x_scale)\n",
        "            resized[y, x] = image[old_y, old_x]\n",
        "\n",
        "    return resized"
      ],
      "metadata": {
        "id": "-VNi5-Q1LZqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rgb_to_grayscale(image):\n",
        "    \"\"\"\n",
        "    Convert RGB image to grayscale\n",
        "    \"\"\"\n",
        "    gray = (GRAY_R_COEFF * image[:, :, 0] +\n",
        "            GRAY_G_COEFF * image[:, :, 1] +\n",
        "            GRAY_B_COEFF * image[:, :, 2])\n",
        "\n",
        "    return gray.astype(np.uint8)"
      ],
      "metadata": {
        "id": "lNKAK7khQdRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(filepath, target_width, target_height):\n",
        "    \"\"\"\n",
        "    Full preprocessing pipeline: load, resize, grayscale\n",
        "    Returns a (H, W) uint8 array.\n",
        "    \"\"\"\n",
        "    # Load RGB image using PIL\n",
        "    img = Image.open(filepath).convert('RGB')\n",
        "    img = np.array(img)\n",
        "\n",
        "    # Resize using nearest-neighbor\n",
        "    img = resize_nearest_neighbor(img, target_width, target_height)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    img = rgb_to_grayscale(img)\n",
        "\n",
        "    # Convert to float32 (for PyTorch)\n",
        "    # We want to keep 0-255 range for INT8 quantization!\n",
        "    img = img.astype(np.float32)\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "_BC88BMsQiiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store preprocessed images and labels\n",
        "all_data = []\n",
        "all_labels = []\n",
        "\n",
        "# Go through each class directory\n",
        "for class_name in class_names:\n",
        "\n",
        "    # Construct path to class folder\n",
        "    class_folder = DATASET_PATH / class_name\n",
        "    image_files = sorted(os.listdir(class_folder))\n",
        "\n",
        "    # Load and preprocess each image\n",
        "    for image_file in image_files:\n",
        "        if image_file.endswith('.BMP'):\n",
        "            image_path = class_folder / image_file\n",
        "\n",
        "            # Preprocess (load, resize, grayscale)\n",
        "            img = preprocess_image(image_path, IMG_WIDTH, IMG_HEIGHT)\n",
        "\n",
        "            # Append to lists\n",
        "            all_data.append(img)\n",
        "            all_labels.append(class_to_idx[class_name])\n",
        "\n",
        "# Summary\n",
        "num_samples = len(all_data)\n",
        "print(f\"Loaded {num_samples} total samples\")\n",
        "print(f\"Image shape: {all_data[0].shape}\")\n",
        "print(f\"Samples per class:\")\n",
        "for class_name in class_names:\n",
        "    count = all_labels.count(class_to_idx[class_name])\n",
        "    print(f\"  {class_name}: {count}\")"
      ],
      "metadata": {
        "id": "t9K3wapnQrzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair each sample with its associated label\n",
        "data_label_pairs = list(zip(all_data, all_labels))\n",
        "\n",
        "# Shuffle the pairs randomly\n",
        "random.shuffle(data_label_pairs)\n",
        "\n",
        "# Unzip back into separate lists\n",
        "shuffled_data, shuffled_labels = zip(*data_label_pairs)\n",
        "shuffled_data = list(shuffled_data)\n",
        "shuffled_labels = list(shuffled_labels)\n",
        "\n",
        "# Calculate split indices\n",
        "test_end_idx = int(TEST_SPLIT * num_samples)\n",
        "val_end_idx = int((VAL_SPLIT + TEST_SPLIT) * num_samples)\n",
        "\n",
        "print(f\"Test end index: {test_end_idx}\")\n",
        "print(f\"Validation end index: {val_end_idx}\")"
      ],
      "metadata": {
        "id": "ey5ueetRTdVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The first section of shuffled samples becomes the test set\n",
        "test_data = shuffled_data[:test_end_idx]\n",
        "test_labels = shuffled_labels[:test_end_idx]\n",
        "\n",
        "# The second section of shuffled samples becomes the validation set\n",
        "val_data = shuffled_data[test_end_idx:val_end_idx]\n",
        "val_labels = shuffled_labels[test_end_idx:val_end_idx]\n",
        "\n",
        "# The third section of shuffled samples becomes the training set\n",
        "train_data = shuffled_data[val_end_idx:]\n",
        "train_labels = shuffled_labels[val_end_idx:]\n",
        "\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "zUgVvirlTwCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a sample (by index) in the training set\n",
        "idx = 0\n",
        "\n",
        "# Get the image and label\n",
        "img = train_data[idx]\n",
        "label = train_labels[idx]\n",
        "\n",
        "# Plot the image\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
        "plt.title(f\"Label: {label} (Class: {class_names[label]})\")\n",
        "plt.xlabel(f\"Shape: {img.shape}, Range: [{img.min()}, {img.max()}]\")\n",
        "plt.colorbar(label='Pixel value')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w1IDxrNoTzNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Custom Dataset and DataLoader\n",
        "\n",
        "We need to wrap our dataset up in a custom Dataset class and provide a way for the training process to fetch samples in batches."
      ],
      "metadata": {
        "id": "9bJD2rwBv4wB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for image data\n",
        "    \"\"\"\n",
        "    def __init__(self, data_list, labels_list):\n",
        "        \"\"\"\n",
        "        Initialize the dataset\n",
        "        \"\"\"\n",
        "        self.data = data_list\n",
        "        self.labels = labels_list\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns one sample at index idx (as PyTorch tensors)\n",
        "        \"\"\"\n",
        "        # Get the image: shape (H, W)\n",
        "        image = self.data[idx]\n",
        "\n",
        "        # Add channel dimension: (H, W) to (1, H, W)\n",
        "        # PyTorch Conv2d expects (C, H, W) format\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        x = torch.FloatTensor(image)\n",
        "        y = torch.LongTensor([self.labels[idx]])[0]\n",
        "\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "NZXTSDQjv4mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap each dataset\n",
        "train_dataset = ImageDataset(train_data, train_labels)\n",
        "val_dataset = ImageDataset(val_data, val_labels)\n",
        "test_dataset = ImageDataset(test_data, test_labels)\n",
        "\n",
        "# Verify\n",
        "x, y = train_dataset[0]\n",
        "print(f\"Sample shape: {x.shape}\")\n",
        "print(f\"Sample dtype: {x.dtype}\")\n",
        "print(f\"Label: {y} (Class: {class_names[y]})\")"
      ],
      "metadata": {
        "id": "lO6pDzmGUIy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataLoader for each of our splits\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Demonstrate how to get one batch from the training DataLoader\n",
        "sample_batch_x, sample_batch_y = next(iter(train_loader))\n",
        "print(f\"Batch input shape: {sample_batch_x.shape}\")\n",
        "print(f\"Batch labels shape: {sample_batch_y.shape}\")"
      ],
      "metadata": {
        "id": "PU7N-Vriwj7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Machine Learning Model\n",
        "\n",
        "We're going to build a simple convolutional neural network (CNN)."
      ],
      "metadata": {
        "id": "1QP6T19FxLqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Convolutional Neural Network for image classification\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_width,\n",
        "        input_height,\n",
        "        conv_1_num_filters,\n",
        "        conv_2_num_filters,\n",
        "        num_classes,\n",
        "        dropout\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Constructor that defines the CNN layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Store input dimensions for reference\n",
        "        self.input_width = input_width\n",
        "        self.input_height = input_height\n",
        "\n",
        "        # Convolutional layer 1\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            1,\n",
        "            conv_1_num_filters,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(conv_1_num_filters)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Convolutional layer 2\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            conv_1_num_filters,\n",
        "            conv_2_num_filters,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(conv_2_num_filters)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Flatten output of convolution layers to 1D array\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Fully connected layer (LazyLinear infers input size automatically)\n",
        "        # One node for each class\n",
        "        self.fc1 = nn.LazyLinear(num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines how data flows through the model\n",
        "        \"\"\"\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Fully connected layer\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "loj4XwoOwzq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = SimpleCNN(\n",
        "    input_width=IMG_WIDTH,\n",
        "    input_height=IMG_HEIGHT,\n",
        "    conv_1_num_filters=CONV_1_NUM_FILTERS,\n",
        "    conv_2_num_filters=CONV_2_NUM_FILTERS,\n",
        "    num_classes=len(class_names),\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "# Move to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Print model details\n",
        "# Note: number of parameters unknown until we do a forward pass\n",
        "print(model)"
      ],
      "metadata": {
        "id": "khz1XcnWxv9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function (measure how wrong the model's predictions are)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (how to adjust the model's weights to reduce loss)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "GJUN8zQA2B27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "We are now ready to train our model!"
      ],
      "metadata": {
        "id": "TroVpZZ32sFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(\n",
        "    model,\n",
        "    dataloader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device\n",
        "):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch\n",
        "    \"\"\"\n",
        "    total_loss = 0.0\n",
        "    num_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Enable training-specific behaviors (e.g. dropout)\n",
        "    model.train()\n",
        "\n",
        "    # Do one full training cycle on a batch of training data\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        # Move data to the same device as the model\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_x)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(outputs, batch_y)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get total loss for the batch (loss.item() is the average)\n",
        "        total_loss += loss.item() * batch_x.size(0)\n",
        "        total_samples += batch_y.size(0)\n",
        "\n",
        "        # Count how many predictions matched the true labels\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        num_correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = num_correct / total_samples\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "05vQm3dJ2mfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader, loss_fn, device):\n",
        "    \"\"\"\n",
        "    Compute the loss and accuracy on a given dataset\n",
        "    \"\"\"\n",
        "    total_loss = 0.0\n",
        "    num_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Disable training-specific behaviors (e.g. dropout)\n",
        "    model.eval()\n",
        "\n",
        "    # Do not track gradients during validation\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in dataloader:\n",
        "            # Move data to the same device as the model\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_x)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(outputs, batch_y)\n",
        "\n",
        "            # Get total loss for the batch (loss.item() is the average)\n",
        "            total_loss += loss.item() * batch_x.size(0)\n",
        "            total_samples += batch_y.size(0)\n",
        "\n",
        "            # Count how many predictions matched the true labels\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            num_correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = num_correct / total_samples\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "5mAFQiBEHo-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store metrics for plotting later\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = validate(\n",
        "        model,\n",
        "        val_loader,\n",
        "        loss_fn,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch [{epoch+1:3d}/{NUM_EPOCHS}] | \"\n",
        "            f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:6.2f} | \"\n",
        "            f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:6.2f}\")\n",
        "\n",
        "# Print model parameter count (now that LazyLinear is initialized)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params}\")\n",
        "print(f\"Trainable parameters: {trainable_params}\")"
      ],
      "metadata": {
        "id": "L8QQJVZUHqw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create plots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot losses\n",
        "ax1.plot(train_losses, label=\"Training Loss\")\n",
        "ax1.plot(val_losses, label=\"Validation Loss\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax1.set_title(\"Training and Validation Loss\")\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot accuracies\n",
        "ax2.plot(train_accuracies, label=\"Training Accuracy\")\n",
        "ax2.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.set_ylabel(\"Accuracy (%)\")\n",
        "ax2.set_title(\"Training and Validation Accuracy\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Show plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0F1GPgAfHxcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model\n",
        "\n",
        "We use the holdout (test) dataset to see how well the model performs."
      ],
      "metadata": {
        "id": "LyuBM6P9OtUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on our test set\n",
        "test_loss, test_accuracy = validate(model, test_loader, loss_fn, device)\n",
        "\n",
        "# Print out the results\n",
        "print(f\"Test Loss: {test_loss:.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "1wIKWOD8Iyic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all predictions and true labels from test set\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "# Put model into evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Do not track gradients\n",
        "with torch.no_grad():\n",
        "\n",
        "    # Get batches of data from the test dataset loader\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        # Get predicted classes (max value of outputs)\n",
        "        outputs = model(batch_x)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Move predictions and labels to CPU\n",
        "        all_predictions.extend(predicted.cpu())\n",
        "        all_labels.extend(batch_y.cpu())\n",
        "\n",
        "# Convert predictions and labels to NumPy arrays\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_labels = np.array(all_labels)"
      ],
      "metadata": {
        "id": "Q5FFewZMOy5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty array for our confusion matrix\n",
        "num_classes = len(class_names)\n",
        "conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "# Add the stats to the confusion matrix\n",
        "for true_label, pred_label in zip(all_labels, all_predictions):\n",
        "    conf_matrix[true_label, pred_label] += 1\n",
        "\n",
        "# Print header\n",
        "print(\"Confusion Matrix:\")\n",
        "print(f\"{'True\\\\Pred':<12}\", end=\"\")\n",
        "for class_name in class_names:\n",
        "    print(f\"{class_name:>12}\", end=\"\")\n",
        "print()\n",
        "\n",
        "# Print matrix with row labels\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"{class_name:<12}\", end=\"\")\n",
        "    for j in range(num_classes):\n",
        "        print(f\"{conf_matrix[i, j]:>12}\", end=\"\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "eED56DosO4LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate per-class accuracy\n",
        "print(\"\\nPer-Class Performance:\")\n",
        "print(f\"  {'Class':<15} {'Correct':<10} {'Total':<10} {'Accuracy':<10}\")\n",
        "print(f\"  {'-'*45}\")\n",
        "\n",
        "# Print per-class accuracies\n",
        "for i, class_name in enumerate(class_names):\n",
        "    correct = conf_matrix[i, i]  # Diagonal elements\n",
        "    total = conf_matrix[i, :].sum()  # Sum of row\n",
        "    accuracy = 100 * correct / total if total > 0 else 0\n",
        "    print(f\"  {class_name:<15} {correct:<10} {total:<10} {accuracy:>6.2f}%\")"
      ],
      "metadata": {
        "id": "_Zag0PeyO5Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print header\n",
        "print(\"Metrics per Class:\")\n",
        "print(f\"  {'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
        "print(f\"  {'-'*51}\")\n",
        "\n",
        "# Calculate per-class metrics\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "for i, class_name in enumerate(class_names):\n",
        "    # True Positives\n",
        "    tp = conf_matrix[i, i]\n",
        "\n",
        "    # False Positives\n",
        "    fp = conf_matrix[:, i].sum() - tp\n",
        "\n",
        "    # False Negatives\n",
        "    fn = conf_matrix[i, :].sum() - tp\n",
        "\n",
        "    # Precision: TP / (TP + FP)\n",
        "    if (tp + fp) > 0:\n",
        "        precision = tp / (tp + fp)\n",
        "    else:\n",
        "        precision = 0\n",
        "\n",
        "    # Recall: TP / (TP + FN)\n",
        "    if (tp + fn) > 0:\n",
        "        recall = tp / (tp + fn)\n",
        "    else:\n",
        "        recall = 0\n",
        "\n",
        "    # F1 score: single metric that combines precision (accuracy of\n",
        "    # positive predictions) and recall (completeness)\n",
        "    if (precision + recall) > 0:\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    else:\n",
        "        f1 = 0\n",
        "\n",
        "    # Store metrics\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Print class metrics\n",
        "    print(f\"  {class_name:<15} {precision:>10.4f}  {recall:>10.4f}  {f1:>10.4f}\")\n",
        "\n",
        "# Calculate simple macro average across all classes\n",
        "macro_precision = np.mean(precisions)\n",
        "macro_recall = np.mean(recalls)\n",
        "macro_f1 = np.mean(f1_scores)\n",
        "\n",
        "# Print overall (average) metrics\n",
        "print(f\"  {'-'*51}\")\n",
        "print(f\"  {'Macro Avg':<15} {macro_precision:>10.4f}  {macro_recall:>10.4f}  {macro_f1:>10.4f}\")\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = np.trace(conf_matrix) / conf_matrix.sum() * 100\n",
        "print(f\"\\n  {'Overall Accuracy':<15} {overall_accuracy:>6.2f}%\")"
      ],
      "metadata": {
        "id": "PfLQIHW9O8Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Sample Inference\n",
        "\n",
        "Demonstrate how the model performs on a single sample."
      ],
      "metadata": {
        "id": "VYGIamh9PYMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose an index (in the test set)\n",
        "idx = 0\n",
        "\n",
        "# Get a sample\n",
        "x, y = test_dataset[idx]\n",
        "\n",
        "# Disable training features\n",
        "model.eval()\n",
        "\n",
        "# Add a batch dimension (the model expects it even if batch=1)\n",
        "x_batch = x.unsqueeze(0)\n",
        "print(f\"Sample shape: {x_batch.shape}\")\n",
        "\n",
        "# Move data to the same device as the model\n",
        "x_batch = x_batch.to(device)\n",
        "\n",
        "# Run inference (no gradient tracking)\n",
        "with torch.no_grad():\n",
        "    output = model(x_batch)\n",
        "\n",
        "# Show inference results\n",
        "print(f\"Ground truth label: {y} (Class: {class_names[y]})\")\n",
        "print(f\"Raw output (logits): {output[0].cpu().numpy()}\")\n",
        "print(f\"Predicted class: {torch.argmax(output[0]).item()}\")\n",
        "\n",
        "# Remove channel dimension for plotting: (1, H, W) to (H, W)\n",
        "x_plot = x.squeeze(0)\n",
        "\n",
        "# Plot the image\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(x_plot, cmap='gray', vmin=0, vmax=255)\n",
        "plt.title(f\"Label: {y} (Class: {class_names[y]})\")\n",
        "plt.xlabel(f\"Shape: {x_plot.shape}, Range: [{x_plot.min()}, {x_plot.max()}]\")\n",
        "plt.colorbar(label='Pixel value')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BktAdhCHPWRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Model\n",
        "\n",
        "Note that in most cases, the export process will produce 2 separate files:\n",
        "* **.onnx** - Model architecture and metadata (with references to external weight data)\n",
        "* **.onnx.data** - Model weights (external data file)"
      ],
      "metadata": {
        "id": "Oo3mVcXlRR_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the model into evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create a dummy input tensor with the same shape as one sample (batch=1)\n",
        "dummy_input = torch.randn(1, 1, IMG_HEIGHT, IMG_WIDTH).to(device)\n",
        "\n",
        "# Export to ONNX\n",
        "torch.onnx.export(\n",
        "    model,                              # Model to export\n",
        "    dummy_input,                        # Example input (for tracing)\n",
        "    ONNX_PATH,                          # Output file path\n",
        "    export_params=True,                 # Export with trained weights\n",
        "    opset_version=ONNX_OPSET_VERSION,   # Which operations are supported\n",
        "    do_constant_folding=True,           # Optimize constant operations\n",
        "    input_names=['input'],              # Name for input layer\n",
        "    output_names=['output']             # Name for output layer\n",
        ")\n",
        "print(f\"Model exported to: {ONNX_PATH}\")"
      ],
      "metadata": {
        "id": "hedKIawWPfni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Calibration Data\n",
        "\n",
        "Export a few samples from the validation set to act as calibration data for post-training quantization."
      ],
      "metadata": {
        "id": "GkyTQ6-DSXge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't exceed the total number of available samples\n",
        "num_samples = min(NUM_CALIB_SAMPLES, len(val_dataset))\n",
        "\n",
        "# Randomly choose from validation set\n",
        "indices = random.sample(range(len(val_dataset)), num_samples)\n",
        "\n",
        "# Get samples (ignore the labels) and convert to NumPy arrays\n",
        "calib_samples = []\n",
        "for i in indices:\n",
        "    x, _ = val_dataset[i]\n",
        "    calib_samples.append(x.numpy())\n",
        "\n",
        "# Stack into a single array: shape (num_samples, 1, H, W)\n",
        "calib_data = np.stack(calib_samples, axis=0)\n",
        "\n",
        "# Save samples as NPZ\n",
        "np.savez(CALIB_NPZ_PATH, input=calib_data)\n",
        "print(f\"Calibration data shape: {calib_data.shape}\")\n",
        "print(f\"Calibration data range: [{calib_data.min():.1f}, {calib_data.max():.1f}]\")\n",
        "print(f\"Saved calibration data to: {CALIB_NPZ_PATH}\")"
      ],
      "metadata": {
        "id": "BOBUUAoRRyaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Helper Code\n",
        "\n",
        "We just need to remember our resolution and grayscale conversion coefficients."
      ],
      "metadata": {
        "id": "MGZ1PAq1Tr6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_code = f\"\"\"\\\n",
        "/* Target image dimensions */\n",
        "#define IMG_WIDTH {IMG_WIDTH}\n",
        "#define IMG_HEIGHT {IMG_HEIGHT}\n",
        "\n",
        "/* Grayscale conversion coefficients */\n",
        "#define GRAY_R_COEFF {GRAY_R_COEFF}f\n",
        "#define GRAY_G_COEFF {GRAY_G_COEFF}f\n",
        "#define GRAY_B_COEFF {GRAY_B_COEFF}f\n",
        "\n",
        "/* Labels */\n",
        "#define NUM_CLASSES {len(class_names)}\n",
        "const char* class_names[] = {{\n",
        "\"\"\"\n",
        "\n",
        "# Construct class names in an array\n",
        "for class_name in class_names:\n",
        "    c_code += f\"    \\\"{class_name}\\\",\\n\"\n",
        "c_code += \"};\"\n",
        "\n",
        "print(c_code)"
      ],
      "metadata": {
        "id": "21mVrSZESw_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Inference Static Sample\n",
        "\n",
        "Compare the output of inference from the original model and the quantized/deployed model on the embedded device."
      ],
      "metadata": {
        "id": "W86rM7xWUKL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a sample from the test set\n",
        "idx = 0\n",
        "\n",
        "# Get the sample\n",
        "x_test, y_test = test_dataset[idx]\n",
        "\n",
        "# Run inference to get expected output\n",
        "model.eval()\n",
        "x_batch = x.unsqueeze(0).to(device)\n",
        "with torch.no_grad():\n",
        "    x_batch = x_test.unsqueeze(0).to(device)\n",
        "    output = model(x_batch)\n",
        "    logits = output[0].cpu().numpy()\n",
        "    probabilities = torch.softmax(output, dim=1)\n",
        "    probabilities = probabilities[0].cpu().numpy()\n",
        "    predicted_class = torch.argmax(output[0]).item()\n",
        "\n",
        "# Print expected results\n",
        "print(f\"Ground truth: {class_names[y_test]}\")\n",
        "print(f\"Class prediction: {class_names[predicted_class]}\")\n",
        "print(f\"Logits: {logits}\")\n",
        "print(f\"Probabilities: {probabilities}\")"
      ],
      "metadata": {
        "id": "Z1AHAc1STyAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get image as numpy array: (1, H, W) to (H, W)\n",
        "sample_img = x.squeeze(0).numpy().astype(np.uint8)\n",
        "\n",
        "# Generate expected logits for test sample\n",
        "c_code = \"\"\"/**\n",
        " * Expected logits:\n",
        "\"\"\"\n",
        "for i, class_name in enumerate(class_names):\n",
        "    c_code += f\" *  {class_name}: {logits[i]:.6f}\\n\"\n",
        "c_code += \" */\\n\"\n",
        "\n",
        "# Initialize array\n",
        "c_code += f\"\"\"\n",
        "#include <stdint.h>\n",
        "\n",
        "/* Image buffer size */\n",
        "#define TEST_INPUT_SIZE {IMG_WIDTH * IMG_HEIGHT}\n",
        "\n",
        "/* Input data (already resized and grayscaled) */\n",
        "const uint8_t test_input[{IMG_HEIGHT * IMG_WIDTH}] = {{\n",
        "\"\"\"\n",
        "\n",
        "# Add the buffer values in rows\n",
        "for row in range(IMG_HEIGHT):\n",
        "    row_values = \", \".join(f\"{int(p):3d}\" for p in sample_img[row, :])\n",
        "    c_code += f\"    {row_values},\\n\"\n",
        "\n",
        "# Close the buffer\n",
        "c_code += \"};\"\n",
        "\n",
        "print(c_code)"
      ],
      "metadata": {
        "id": "U-uUKwexUsWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy!\n",
        "\n",
        "Download your *model.onnx* and *model.onnx.data* files. Use your vendor's toolset (e.g. RUHMI) to quantize, compress, and compile the model for your target device."
      ],
      "metadata": {
        "id": "x7FKtKq2VQcn"
      }
    }
  ]
}